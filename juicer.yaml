# Stand configuration file
juicer:
    funciona: ${true}
    debug: true
    minion:
        terminate_after_run: false
        libprocess_port_range: [41000, 41500]
        libprocess_advertise_ip: 150.164.203.201
    servers:
        database_url: mysql+pymysql://lemon:ju1c3@mysql3.ctweb.inweb.org.br:33060/lemonade
        #redis_url: redis://beta.ctweb.inweb.org.br:6379
        redis_url: redis://localhost:6379
    services:
        stand:
            url: http://localhost:3323
            # url: https://dev.ctweb.inweb.org.br/stand
            auth_token: 123456
        tahiti:
            url: http://localhost:5000
            # url: https://demovldb.ctweb.inweb.org.br/tahiti
            # url: https://teste.ctweb.inweb.org.br/tahiti
            # url: https://dev.ctweb.inweb.org.br/tahiti
            # url: https://dm.ctweb.inweb.org.br/tahiti
            # url: https://enap.ctweb.inweb.org.br/tahiti
            auth_token: 123456
        limonero:
            url: http://localhost:3321
            # url: https://dev.ctweb.inweb.org.br/limonero
            # url: https://enap.ctweb.inweb.org.br/limonero
            # url: https://dm.ctweb.inweb.org.br/limonero
            # url: https://demovldb.ctweb.inweb.org.br/limonero
            # url: https://teste.ctweb.inweb.org.br/limonero
            auth_token: 123456
        caipirinha:
            # url: http://beta.ctweb.inweb.org.br/caipirinha
            url: https://dev.ctweb.inweb.org.br/caipirinha
            # url: https://dm.ctweb.inweb.org.br/caipirinha
            # url: https://teste.ctweb.inweb.org.br/caipirinha
            # url: http://localhost:3324
            auth_token: 123456
            storage_id: 2
    config:
        tmp_dir: /tmp
    spark:
        # For more information, see http://spark.apache.org/docs/latest/configuration.html
        # spark.kryoserializer.buffer: 256m
        spark.executor.memory: 1g
        spark.driver.memory: 2g
        # spark.executor.cores: 1
        spark.driver.host: artemis.speed.dcc.ufmg.br
        # spark.sql.catalogImplementation: in-memory
        spark.cores.max: 72
        # spark.driver.memory: 4g
        # Allowed URLS: 
        # local:                Run locally with 1 worker thread
        # local[k]:             Run locally with k worker threads
        # local[*]:             Run locally with as many worker threads as logical cores
        # spark://host:port:    Connect to Spark standalone cluster (default port 7077)
        # mesos://host:port:    Connect to Mesos cluster (default port 5050)
        # yarn:                 Connect to Yarn cluster. Cluster location will be found based 
        #                       on the HADOOP_CONF_DIR or YARN_CONF_DIR variable.
        #spark.master: spark://172.16.203.201:7077 
        spark.master: local[*] #spark://artemis:7077 #local[*]  
        # spark.master: mesos://172.16.203.204:5050
        #spark.executor.uri: hdfs://spark01.ctweb.inweb.org.br:9000/spark/spark-2.2.0-bin-hadoop2.6.tgz
        
        # Dynamic allocation of workers
        # spark.dynamicAllocation.enabled: true
        # spark.shuffle.service.enabled: true

        # spark.io.compression.code: snappy
        # spark.rdd.compress: true

        # Logging
        # spark.eventLog.enabled: true
        # spark.eventLog.dir: hdfs://spark01.ctweb.inweb.org.br:9000/lemonade-spark-events
        # spark.history.fs.logDirectory: hdfs://artemis.speed.dcc.ufmg.br:9000/lemonade-spark-events

        spark.localdir: /tmp
        # spark.submit.deployMode: client
        spark.driver.extraClassPath:
            "/scratch/walter/juicer/libs/BigseaSparkGeomatching.jar.old:\
            /scratch/walter/lemonade-spark-ext/target/lemonade-spark-ext-1.0-SNAPSHOT.jar"
        # spark.jars.packages: master:spark-stemming_2.10:0.1.2
        # Mesos related settings
        # spark.executor.uri: http://beta.ctweb.inweb.org.br/publico/spark-2.2.0-bin-hadoop2.6.tgz
        spark.mesos.fetcherCache.enable: true
        spark.sql.execution.arrow.enabled: true
        ## If running on docker ##
        # spark.mesos.containerizer: docker
        # spark.mesos.executor.docker.image: p7hb/docker-spark
        # spark.mesos.executor.home: /opt/spark-2.2.1-bin-hadoop2.6
        # spark.mesos.rejectOfferDuration: 30s
        # spark.mesos.task.labels: lemonade:spark
        # spark.mesos.driver.failoverTimeout: 30


        # Performance
        # spark.serializer: org.apache.spark.serializer.KryoSerializer

        #HDFS: prefix with spark.hadoop
        spark.hadoop.dfs.client.use.datanode.hostname: true

        spark.sql.session.timeZone: GMT
