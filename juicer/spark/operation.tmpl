#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Auto-generated Spark code from Lemonade Workflow
(c) Speed Labs - Departamento de Ciência da Computação
    Universidade Federal de Minas Gerais
More information about Lemonade to be provided
"""
from concurrent.futures import ThreadPoolExecutor
import json
import os
import re
import string
import sys
import time
import threading
import unicodedata

from textwrap import dedent
from timeit import default_timer as timer

from pyspark.ml import classification, evaluation, feature, tuning, clustering
from pyspark.sql import functions, types

from pyspark.ml import Pipeline
from pyspark.ml.classification import *
from pyspark.ml.clustering import *
from pyspark.ml.evaluation import *
from pyspark.ml.feature import *
from pyspark.ml.tuning import *
from pyspark.ml.recommendation import *
from pyspark.ml.regression import *


from juicer.util import dataframe_util

executor = ThreadPoolExecutor(max_workers={{instances|length}})
submission_lock = threading.Lock()
task_futures = {}


# We need the following code in order to integrate this application log with the
# spark standard log4j. Thus, we set *__SPARK_LOG__* at most one time per
# execution.
__SPARK_LOG__ = None


def spark_logging(spark_session):
    global __SPARK_LOG__
    if not __SPARK_LOG__:
        # noinspection PyProtectedMember
        logger = spark_session.sparkContext._jvm.org.apache.log4j
        __SPARK_LOG__ = logger.LogManager.getLogger(__name__)
    return __SPARK_LOG__


# reload(sys)
# sys.setdefaultencoding('utf8')


def take_sample(df, size=100, default=None):
    """
    Takes a sample from data frame.
    """
    result = default or []
    if hasattr(df, 'take'):
        # result = df.limit(size).toJSON().collect()
        header = ','.join([f.name for f in df.schema.fields])
        result = [header]
        result.extend(
            df.limit(size).rdd.map(dataframe_util.convert_to_csv).collect())
    return result


def juicer_debug(spark_session, name, variable, data_frame):
    """ Debug code """
    spark_logging(spark_session).debug('#' * 20)
    spark_logging(spark_session).debug('|| {} ||'.format(name))
    spark_logging(spark_session).debug('== {} =='.format(variable))
    data_frame.show()
    schema = data_frame.schema
    for attr in schema:
        spark_logging(spark_session).debug('{} {} {} {}'.format(
            attr.name, attr.dataType, attr.nullable, attr.metadata))
{#% autopep8 %#}
{%- for instance in instances %}
{%-  handleinstance instance %}
{%- if instance.has_code %}

# noinspection PyUnusedLocal
def {{instance.parameters.task.operation.slug.replace('-', '_')}}_{{instance.order}}(spark_session, cached_state, emit_event):
    {%- if instance.parameters.task.forms.comment and instance.parameters.task.forms.comment.value %}
    """
    {{instance.parameters.task.forms.comment.value.strip().replace('"', '')}}
    """
    {%- else %}
    """ Operation {{instance.parameters.task.id }} """
    {%- endif %}

    # First we verify whether this task's result is cached
    {%- if instance.supports_cache and False %}
    if '{{instance.parameters.task.id}}' in cached_state:
        emit_event(name='update task', message='Task running',
                   status='RUNNING',
                   identifier='{{instance.parameters.task.id}}')
        spark_logging(spark_session).info ('Cache hit for operation {}'.format('{{instance.parameters.task.id}}'))
        emit_event(name='update task', message='Task completed',
                   status='COMPLETED',
                   identifier='{{instance.parameters.task.id}}')
        return cached_state.get('{{instance.parameters.task.id}}')
    {%- endif %}

    spark_logging(spark_session).info(
        "Submitting parent task {{instance.parameters.task.parents}} "
        "before {{instance.parameters.task.id}}")

    # If the task's result is not cached, we submit its dependencies first
    {%- for parent in instance.parameters.task.parents %}
    {%- set parent_instance = instances_by_task_id[parent] %}
    {%- set is_satisfied = dependency_controller.is_satisfied(parent_instance.parameters.task.id) %}
    {%- if parent_instance.has_code and is_satisfied %}
    {%- set s = dependency_controller.satisfied(parent_instance.parameters.task.id) %}
    with submission_lock:
        if '{{parent}}' not in task_futures:
            task_futures['{{parent}}'] = executor.\
                submit(
                    lambda: {{parent_instance.parameters.task.operation.slug.replace('-', '_')}}_{{parent_instance.order}}(
                        spark_session, cached_state, emit_event)
                )

    {%- endif %}
    {%- endfor %}

    # Next we wait for the dependencies to complete
    {%- for parent in instance.parameters.task.parents %}
    {%- set parent_instance = instances_by_task_id[parent] %}
    {%- set is_satisfied = dependency_controller.is_satisfied(parent_instance.parameters.task.id) %}
    {%- if parent_instance.has_code and is_satisfied %}
    {%- set s = dependency_controller.satisfied(parent_instance.parameters.task.id) %}
    {% if parent_instance.get_output_names(", ") %}
    parent_result = task_futures['{{parent}}'].result()
    {%- for port_name,out in zip(parent_instance.parameters.task.port_names, parent_instance.get_output_names(',').split(','))%}
    {{out}}, pr_{{out}} = (parent_result['{{port_name}}']['output'], parent_result['{{port_name}}']['sample']){% endfor %}{% endif %}
    ts_{{parent_instance.output}} = parent_result['time']
    {%- endif %}
    {%- endfor %}
    
    spark_logging(spark_session).info(
        'All parents completed, submitting {{instance.parameters.task.id}}')
 
    emit_event(name='update task', message='Task running',
               status='RUNNING',
               identifier='{{instance.parameters.task.id}}')
  
    spark_logging(spark_session).info(
        'Lemonade task {{instance.parameters.task.id}} started') 
  
    start = timer()
    {{instance.generate_code().strip() | indent(width=4, indentfirst=False)}}
    {%- set outputs = instance.get_data_out_names('|').split('|') %}
    {%- if instance.parameters.get('logging', {}).get('log_level') == 'DEBUG' %}
    {%- if outputs %}
    {%- for variable in outputs %}
    juicer_debug(spark_session, '{{instance.__class__.__name__}}', '{{variable}}', {{variable}})
    {%- endfor %}
    {%- endif %}
    {%- endif %}

    {%- for gen_result in instance.get_generated_results() %}
    emit_event(name='task result', message='{{gen_result.type}}',
               status='COMPLETED',
               identifier='{{instance.parameters.task.operation.id}}/{{instance.parameters.task.id}}')
    {%- endfor %}

    results = {
      {%- set is_leaf = instance.out_degree == 0 %}
      {%- for port_name,out in zip(instance.parameters.task.port_names, instance.get_output_names(',').split(',')) %}
        {%- if port_name and out %}
         '{{port_name}}': {'output': {{out}}, 'sample': {%- if is_leaf %} take_sample({{out}}) {%- else %} [] {%- endif %}},
        {%- endif %}
      {%- endfor %}
    }

    time_elapsed = timer() - start
    emit_event(name='update task', message='Task completed',
               status='COMPLETED',
               identifier='{{instance.parameters.task.id}}')

    spark_logging(spark_session).info(
        'Lemonade task {{instance.parameters.task.id}} completed')
    results.update({'time': time_elapsed})
    return results

{%- endif %}
{%- endhandleinstance %}
{% endfor %}

def main(spark_session, cached_state, emit_event):
    """ Run generated code """

    start = time.time()

    session_start_time = time.time()
    
    {%- for instance in instances %}
    {%- if instance.has_code %}
    {%- if instance.multiple_inputs %}
    {{instance.get_inputs_names.replace(',', '=') }} = None
    {% endif %}
    {%- endif %}
    {%- endfor %}

    {%- for instance in instances %}
    {%- set is_satisfied = dependency_controller.is_satisfied(instance.parameters.task.id) %}
    {%- if instance.has_code and is_satisfied and instance.out_degree == 0 %}
    {%- set s = dependency_controller.satisfied(instance.parameters.task.id) %}
    task_futures['{{instance.parameters.task.id}}'] = executor.submit(
        lambda: {{instance.parameters.task.operation.slug.replace('-', '_')}}_{{instance.order}}(spark_session, cached_state, emit_event))
    {%- endif %}
    {%- endfor %}
    
    {%- for instance in instances %}
    {%- set is_satisfied = dependency_controller.is_satisfied(instance.parameters.task.id) %}
    {%- if instance.has_code and is_satisfied and instance.out_degree == 0 %}
    {%- set s = dependency_controller.satisfied(instance.parameters.task.id) %}
    task_futures['{{instance.parameters.task.id}}'].result()
    {%- endif %}
    {%- endfor %}

    end = time.time()
    print("{}\t{}".format(end - start, end - session_start_time))
    return {
        'status': 'OK',
        'message': 'Execution defined',
        {%- for instance in instances %}
          {%- set is_satisfied = dependency_controller.is_satisfied(instance.parameters.task.id) %}
          {%- if instance.has_code and is_satisfied %}
            '{{instance.parameters.task.id}}':
                task_futures['{{instance.parameters.task.id}}'].result(),
          {%- endif %}
        {%- endfor %}
    }
{%- if execute_main %}

def dummy_emit_event(room, namespace):
    def _dummy_emit_event(name, message, status, identifier, **kwargs):
        return None
    return _dummy_emit_event

from pyspark.sql import SparkSession
spark_session = SparkSession.builder.getOrCreate()
spark_session.sparkContext.setLogLevel('INFO')
main(spark_session, {}, dummy_emit_event(room=-1, namespace='/none'))

{%- endif %}
{#% endautopep8 %#}
