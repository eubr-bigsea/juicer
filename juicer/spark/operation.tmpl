# -*- coding: utf-8 -*-
#!/usr/bin/env python
"""
Auto-generated Spark code from Lemonade Workflow
(c) Speed Labs - Departamento de Ciência da Computação
    Universidade Federal de Minas Gerais
More information about Lemonade to be provided
"""
import json
import os
import string
import sys
import time
import unicodedata

from timeit import default_timer as timer

from pyspark.ml import classification, evaluation, feature, tuning, clustering
from pyspark.sql import functions, types
from pyspark.sql.functions import *
from pyspark.sql.types import *

from pyspark.ml import Pipeline
from pyspark.ml.classification import *
from pyspark.ml.clustering import *
from pyspark.ml.evaluation import *
from pyspark.ml.feature import *
from pyspark.ml.tuning import *
from pyspark.ml.recommendation import *

# from pyspark.sql.functions import *
# from pyspark.sql.types import *

# We need the following code in order to integrate this application log with the
# spark standard log4j. Thus, we set *__SPARK_LOG__* at most one time per execution.
__SPARK_LOG__ = None
def sparkLogging(spark_session):
    global __SPARK_LOG__
    if not __SPARK_LOG__:
       log4jLogger = spark_session.sparkContext._jvm.org.apache.log4j
       __SPARK_LOG__ = log4jLogger.LogManager.getLogger(__name__)
    return __SPARK_LOG__


reload(sys)
sys.setdefaultencoding('utf8')

def juicer_debug(spark_session, name, variable, data_frame):
    """ Debug code """
    sparkLogging(spark_session).debug('#' * 20)
    sparkLogging(spark_session).debug('|| {} ||'.format(name))
    sparkLogging(spark_session).debug('== {} =='.format(variable))
    data_frame.show()
    schema = data_frame.schema
    for attr in schema:
        sparkLogging(spark_session).debug('{} {} {} {}'.format(attr.name, attr.dataType, attr.nullable, attr.metadata))
{#% autopep8 %#}
{%- for instance in instances %}
{%- if instance.has_code %}
def {{instance.parameters.task.operation.slug.replace('-', '_')}}_gen_{{instance.get_output_names('_')}}(
        spark_session, cached_state, emit_event
        {%- if instance.get_inputs_names %}, {{instance.get_inputs_names}}{% endif %}):
    {%- if instance.parameters.task.forms.comment and instance.parameters.task.forms.comment.value.strip()%}
    """
    {{instance.parameters.task.forms.comment.value.replace('"', '')}}
    """
    {%- else %}
    """ Operation {{instance.parameters.task.id }} """
    {%- endif %}

    emit_event(name='update task', msg='Task running',
        status='RUNNING', identifier='{{instance.parameters.task.id}}')

    if '{{instance.parameters.task.id}}' in cached_state:
        sparkLogging(spark_session).info ('Cache hit for operation {}'.format('{{instance.parameters.task.id}}'))
        emit_event(name='update task', msg='Task completed',
           status='COMPLETED', identifier='{{instance.parameters.task.id}}')
        return cached_state.get('{{instance.parameters.task.id}}')

    start = timer()
    {{instance.generate_code().strip() | indent(width=4, indentfirst=False)}}
    {%- set outputs = instance.get_data_out_names('|').split('|') %}
    {%- if instance.parameters.get('logging', {}).get('log_level') == 'DEBUG' %}
    {%- if outputs %}
    {%- for variable in outputs %}
    juicer_debug(spark_session, '{{instance.__class__}}', '{{variable}}', {{variable}})
    {%- endfor %}
    {%- endif %}
    {%- endif %}

    results = (
      {%- set is_leaf = instance.out_degree == 0 %}
      {%- for out in instance.get_output_names(',').split(',') %}
         {{out}}, {%- if is_leaf %} {{out}}.take(100) if hasattr({{out}}, 'take') else [] {%- else %} [] {%- endif %},
      {%- endfor %}
    )

    time_elapsed = timer() - start
    emit_event(name='update task', msg='Task completed',
       status='COMPLETED', identifier='{{instance.parameters.task.id}}')
    return {% if instance.get_output_names() %} results + {% endif %} (time_elapsed,)

{%- endif %}
{% endfor %}

def main(spark_session, cached_state, emit_event):
    """ Run generated code """

    start = time.time()

    session_start_time = time.time()
    # spark_session.sparkContext.addPyFile('/tmp/dist.zip')

    # Declares and initializes variables in order to do not generate NameError.
    # Some tasks may not generate code, but at least one of their outputs is
    # connected to a valid input in a task generating code. This happens when
    # task has a port with multiplicity MANY
    {%- for instance in instances %}
    {%- if instance.has_code %}
    {%- if instance.multiple_inputs %}
    {{instance.get_inputs_names.replace(',', '=') }} = None
    {% endif %}
    {%- endif %}
    {%- endfor %}

    {%- for instance in instances %}
    {%- set is_satisfied = dependency_controller.is_satisfied(instance.parameters.task.id) %}
    {%- if instance.has_code and is_satisfied %}
    {%- set s = dependency_controller.satisfied(instance.parameters.task.id) %}
    {%- if is_satisfied %}
    {% if instance.get_output_names(", ") %}{%- for out in instance.get_output_names(',').split(',')%}{{out}}, pr_{{out}}, {% endfor %}{% endif %}ts_{{instance.output}} = {{instance.parameters.task.operation.slug.replace('-', '_')}}_gen_{{instance.get_output_names('_')}}(spark_session, cached_state, emit_event
      {%- if instance.get_inputs_names %}, {{instance.get_inputs_names}}{% endif %})
    {%- endif %}
    {%- else %}
    # Task **{{instance.parameters.task.operation.name}}** did not genarate code
    {%- endif %}
    {%- endfor %}


    end = time.time()
    print "{}\t{}".format(end - start, end - session_start_time)
    return {
        'status': 'OK',
        'message': 'Execution defined',
        {%- for instance in instances %}
          {%- set is_satisfied = dependency_controller.is_satisfied(instance.parameters.task.id) %}
          {%- if instance.has_code and is_satisfied %}
            '{{instance.parameters.task.id}}': (
              {%- if instance.get_output_names() %}
                {%- for variable in instance.get_output_names(', ').split('|') -%}
                  {{variable}}, pr_{{variable}},
                {%- endfor -%}
              {% endif -%}
              ts_{{instance.output}}),
          {%- endif %}
        {%- endfor %}
    }
{%- if execute_main %}

def dummy_emit_event(room, namespace):
    def _dummy_emit_event(name, msg, status, identifier):
        return None
    return _dummy_emit_event

from pyspark.sql import SparkSession
spark_session = SparkSession.builder.getOrCreate()
main(spark_session, {}, dummy_emit_event(room=-1, namespace='/none'))

{%- endif %}
{#% endautopep8 %#}
