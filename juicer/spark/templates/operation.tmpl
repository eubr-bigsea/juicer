#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Auto-generated Spark code from Lemonade Workflow
(c) Speed Labs - Departamento de Ciência da Computação
    Universidade Federal de Minas Gerais
More information about Lemonade to be provided
"""
from concurrent.futures import ThreadPoolExecutor
import collections
import functools
import datetime
import itertools
import json
import os
import re
import simplejson
import string
import sys
import time
import threading
import traceback
import unicodedata
import juicer.spark.ext as juicer_ext

from textwrap import dedent
from timeit import default_timer as timer

from pyspark.ml import classification, evaluation, feature, tuning, clustering
from pyspark.sql import functions, types, Row, DataFrame
from pyspark.sql.utils import IllegalArgumentException
from pyspark.sql.window import Window
from pyspark.ml.linalg import Vectors, VectorUDT

from pyspark.ml import Pipeline, PipelineModel
from pyspark.ml.classification import *
from pyspark.ml.clustering import *
from pyspark.ml.evaluation import *
from pyspark.ml.feature import *
from pyspark.ml.tuning import *
from pyspark.ml.recommendation import *
from pyspark.ml.regression import *
from pyspark.mllib.evaluation import *
from juicer import privaaas
from juicer.util import dataframe_util, get_emitter
from juicer.spark.reports import *
from juicer.spark.util import assemble_features_pipeline_model
from juicer.spark.ml_operation import ModelsEvaluationResultList
from juicer.spark.custom_library import *

executor = ThreadPoolExecutor(max_workers={{instances|length}})
submission_lock = threading.Lock()
task_futures = {}

{% for auxiliary_code in transpiler.get_auxiliary_code(instances) %}
{%- include auxiliary_code with context %}
{%- endfor %}

{%- for instance in instances %}
{%-  handleinstance instance %}
{%- if instance.has_code and instance.enabled %}
{%- set task = instance.parameters.task %}
{%- set task_id = task.id %}

# noinspection PyUnusedLocal
def {{task.operation.slug.replace('-', '_')}}_{{instance.order}}(spark_session, cached_state, emit_event):
    """
    {%- if task.forms.comment and task.forms.comment.value %}
    {{task.forms.comment.value.strip().replace('"', '')}}
    {%- else %}
    Operation {{task_id }}
    {%- endif %}
    Task hash: {{instance.parameters.hash}}.
    """
    task_id = '{{task_id}}'
    {%- if task.parents %}
    juicer_ext.spark_logging(spark_session).info(
        """Submitting parent task(s)
        {{task.parents|join(', \n        "')}}
        before {{task.id}}""")

    # If the task's result is not cached, we submit its dependencies first
    {%- set parents = transpiler._get_parent_tasks(instances_by_task_id, instance) %}
    {%- for parent_id, method in parents %}
    parent_id = '{{parent_id}}'
    with submission_lock:
        if parent_id not in task_futures:
            task_futures[parent_id] = executor.submit(
                lambda: {{method}}(spark_session, cached_state, emit_event))
    {%- endfor %}
    {%- endif %}

    {%- if parents %}
    # Next we wait for the dependencies to complete
    {%- for parent_id, method in parents %}
    {%- set parent_instance = instances_by_task_id[parent_id] %}
    {%- if parent_instance.get_output_names(", ") %}
    parent_result = task_futures['{{parent_id}}'].result()
    {%- for port_name,out in zip(parent_instance.parameters.task.port_names, parent_instance.get_output_names(',').split(','))%}
    {{out}} = parent_result['{{port_name}}']
    {%- endfor %}
    ts_{{parent_instance.output}} = parent_result['time']
    {% endif %}
    {%- endfor %}
    juicer_ext.spark_logging(spark_session).info(
        'Parents completed, submitting {}.'.format(task_id))
    {%- endif %}

    {%- if not plain %}
    emit_task_running(task_id, spark_session, emit_event)
    {%- endif %}

    start = timer()
    # First we verify whether this task's result is cached.
    results = {% if instance.supports_cache -%}
    get_cached_state(
        task_id, cached_state, emit_event, spark_session,
        '{{instance.parameters.hash}}')
    {% else %} None
    {%- endif %}
    if results is None:
        # --- Begin operation code ---- #
        {{instance.generate_code().strip() | indent(width=8, indentfirst=False)}}
        # --- End operation code ---- #
        {%- if not plain %}
        {%- for gen_result in instance.get_generated_results() %}
        emit_event(name='task result', message=_('{{gen_result.type}}'),
                   status='COMPLETED',
                   identifier='{{task.operation.id}}/{{task_id}}')
        {%- endfor %}
        {%- endif %}

        {#-
            Privacy related part.
        #}
        {%- set traces = instance.attribute_traceability() %}
        {%- if traces %}
        if dataframe_util.spark_version(spark_session) >= (2, 2, 0):
        {%- for trace in traces %}
        {%- for out in instance.get_data_out_names(',').split(',') %}
        {%- if out %}
            schema = {{trace.input}}.schema
            meta = {}
            {%- if trace.derived_from != '*' %}
            meta.update(schema[str('{{trace.derived_from}}')].metadata)
            {%- endif %}
            {{out}} = {{out}}.withColumn('{{trace.attribute}}',
                functions.col('{{trace.attribute}}').alias('', metadata=meta))
        {%- endif %}
        {%- endfor %}
        {%- endfor %}
            pass
        {%- endif %}

        results = {
            'execution_date': datetime.datetime.utcnow(),
            'task_name': '{{task.name}}',
          {%- set is_leaf = instance.out_degree == 0 %}
          {%- for port_name, out in zip(task.port_names, instance.get_output_names(',').split(',')) %}
            {%- if port_name and out %}
            '{{port_name}}': {{out}},
            {%- endif %}
          {%- endfor %}
        }

    {%- if instance.contains_results() %}
    df_types = (DataFrame, dataframe_util.LazySparkTransformationDataframe)
    outputs = [(name, out) for name, out in results.items()
        if isinstance(out, df_types)]
    {%- if instance.has_code and instance.enabled and instance.contains_sample %}
    for name, out in outputs:
        dataframe_util.emit_sample(task_id, out, emit_event, name)
    {%- endif %}
    {%- if instance.has_code and instance.enabled and instance.contains_schema %}
    for name, out in outputs:
        dataframe_util.emit_schema(task_id, out, emit_event, name)
    {%- endif %}
    {%- endif %}

    {%- if not plain %}
    emit_task_completed(task_id, spark_session, emit_event)
    {%- endif %}

    results['time'] = timer() - start
    return results

{%- endif %}
{%- endhandleinstance %}
{% endfor %}

def emit_task_running(task_id, spark_session, emit_event):
    emit_event(name='update task', message=_('Task running'), status='RUNNING',
               identifier=task_id)
    juicer_ext.spark_logging(spark_session).info(
        'Lemonade task {} started'.format(task_id))

def emit_task_completed(task_id, spark_session, emit_event):
    emit_event(name='update task', message=_('Task completed'),
               status='COMPLETED', identifier=task_id)
    juicer_ext.spark_logging(spark_session).info(
        'Lemonade task {} completed'.format(task_id))


def get_results(_task_futures, task_id):
    return _task_futures[task_id].result() if task_id in _task_futures else None

def get_cached_state(task_id, cached_state, emit_event, spark_session,
                     task_hash):
    results = None
    if task_id in cached_state:
        cached, _hash = cached_state.get(task_id)
        if _hash == task_hash:
            emit_event(name='update task',
                message=_('Task running (cached data)'), status='RUNNING',
                identifier=task_id)
            juicer_ext.spark_logging(spark_session).info(
                'Cache hit for operation {}'.format(task_id))
            results = cached
    return results

def main(spark_session, cached_state, emit_event):
    """ Run generated code """

    try:
        {%- for instance in instances %}
        {%- if instance.has_code and instance.enabled and instance.multiple_inputs %}
        {{instance.get_inputs_names.replace(',', '=') }} = None
        {%- endif %}
        {%- endfor %}

        {%- set ids_and_methods = transpiler.get_ids_and_methods(instances) %}
        {%- for task_id, method in ids_and_methods.items() %}
        task_futures['{{task_id}}'] = executor.submit(
            lambda: {{method}}(spark_session, cached_state, emit_event))
        {%- endfor %}

        {%- for task_id in ids_and_methods.keys() %}
        {%- set s = dependency_controller.satisfied(task_id) %}
        task_futures['{{task_id}}'].result()
        {%- endfor %}
        {%- for disabled in transpiler.get_disabled_tasks(instances) %}
        emit_event(name='update task', message=_(
            'Task completed, but not executed (not used in the workflow).'),
            status='COMPLETED', level='WARN',
            identifier='{{disabled.parameters.task.id}}')
        {%- endfor %}

        {%- for disabled in disabled_tasks %}
        emit_event(name='update task', message=_(
            'Task completed, but not executed (not used in the workflow).'),
            status='COMPLETED', identifier='{{disabled}}')
        {%- endfor %}

        return {
            'status': 'OK',
            'message': 'Execution defined',
            {%- for instance in transpiler._get_enabled_tasks(instances) %}
            '{{instance.parameters.task.id}}':
                [get_results(task_futures,
                '{{instance.parameters.task.id}}'),
                '{{instance.parameters.hash}}'],
            {%- endfor %}
        }
    except Exception as e:
        spark_session.sparkContext.cancelAllJobs()
        traceback.print_exc(file=sys.stderr)
        if not dataframe_util.handle_spark_exception(e):
            raise

{%- if execute_main %}

def dummy_emit_event(room, namespace):
    def _dummy_emit_event(name, message, status, identifier, **kwargs):
        return None
    return _dummy_emit_event

from pyspark.sql import SparkSession
spark_session = SparkSession.builder.getOrCreate()
spark_session.sparkContext.setLogLevel('INFO')
main(spark_session, {}, dummy_emit_event(room=-1, namespace='/none'))

{%- endif %}
