#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Auto-generated Spark code from Lemonade Workflow
(c) Speed Labs - Departamento de Ciência da Computação
    Universidade Federal de Minas Gerais
More information about Lemonade to be provided
"""
from concurrent.futures import ThreadPoolExecutor
import itertools
import json
import os
import re
import string
import sys
import time
import threading
import unicodedata

from textwrap import dedent
from timeit import default_timer as timer

from pyspark.ml import classification, evaluation, feature, tuning, clustering
from pyspark.sql import functions, types
from pyspark.sql.utils import IllegalArgumentException

from pyspark.ml import Pipeline
from pyspark.ml.classification import *
from pyspark.ml.clustering import *
from pyspark.ml.evaluation import *
from pyspark.ml.feature import *
from pyspark.ml.tuning import *
from pyspark.ml.recommendation import *
from pyspark.ml.regression import *

from juicer.util import dataframe_util

executor = ThreadPoolExecutor(max_workers={{instances|length}})
submission_lock = threading.Lock()
task_futures = {}


# We need the following code in order to integrate this application log with the
# spark standard log4j. Thus, we set *__SPARK_LOG__* at most one time per
# execution.
__SPARK_LOG__ = None


def spark_logging(spark_session):
    global __SPARK_LOG__
    if not __SPARK_LOG__:
        # noinspection PyProtectedMember
        logger = spark_session.sparkContext._jvm.org.apache.log4j
        __SPARK_LOG__ = logger.LogManager.getLogger(__name__)
    return __SPARK_LOG__



def take_sample(df, size=100, default=None):
    """
    Takes a sample from data frame.
    """
    result = default or []
    if hasattr(df, 'take'):
        # result = df.limit(size).toJSON().collect()
        header = ','.join([f.name for f in df.schema.fields])
        result = [header]
        result.extend(
            df.limit(size).rdd.map(dataframe_util.convert_to_csv).collect())
    return result


def juicer_debug(spark_session, name, variable, data_frame):
    """ Debug code """
    spark_logging(spark_session).debug('#' * 20)
    spark_logging(spark_session).debug('|| {} ||'.format(name))
    spark_logging(spark_session).debug('== {} =='.format(variable))
    data_frame.show()
    schema = data_frame.schema
    for attr in schema:
        spark_logging(spark_session).debug('{} {} {} {}'.format(
            attr.name, attr.dataType, attr.nullable, attr.metadata))
{#% autopep8 %#}
{%- for instance in instances %}
{%-  handleinstance instance %}
{%- if instance.has_code %}

# noinspection PyUnusedLocal
def {{instance.parameters.task.operation.slug.replace('-', '_')}}_{{instance.order}}(spark_session, cached_state, emit_event):
    {%- if instance.parameters.task.forms.comment and instance.parameters.task.forms.comment.value %}
    """
    {{instance.parameters.task.forms.comment.value.strip().replace('"', '')}}
    """
    {%- else %}
    """ Operation {{instance.parameters.task.id }} """
    {%- endif %}

    {%- if instance.supports_cache and False %}
    # First we verify whether this task's result is cached
    if '{{instance.parameters.task.id}}' in cached_state:
        emit_event(name='update task', message='Task running',
                   status='RUNNING',
                   identifier='{{instance.parameters.task.id}}')
        spark_logging(spark_session).info ('Cache hit for operation {}'.format('{{instance.parameters.task.id}}'))
        emit_event(name='update task', message='Task completed',
                   status='COMPLETED',
                   identifier='{{instance.parameters.task.id}}')
        return cached_state.get('{{instance.parameters.task.id}}')
    {%- endif %}

    spark_logging(spark_session).info(
        "Submitting parent task {{instance.parameters.task.parents}} "
        "before {{instance.parameters.task.id}}")

    # If the task's result is not cached, we submit its dependencies first
    {%- for parent in instance.parameters.task.parents %}
    {%- set parent_instance = instances_by_task_id[parent] %}
    {%- set is_satisfied = dependency_controller.is_satisfied(parent_instance.parameters.task.id) %}
    {%- if parent_instance.has_code and is_satisfied %}
    {%- set s = dependency_controller.satisfied(parent_instance.parameters.task.id) %}
    with submission_lock:
        if '{{parent}}' not in task_futures:
            task_futures['{{parent}}'] = executor.\
                submit(
                    lambda: {{parent_instance.parameters.task.operation.slug.replace('-', '_')}}_{{parent_instance.order}}(
                        spark_session, cached_state, emit_event)
                )

    {%- endif %}
    {%- endfor %}

    # Next we wait for the dependencies to complete
    {%- for parent in instance.parameters.task.parents %}
    {%- set parent_instance = instances_by_task_id[parent] %}
    {%- set is_satisfied = dependency_controller.is_satisfied(parent_instance.parameters.task.id) %}
    {%- if parent_instance.has_code and is_satisfied %}
    {%- set s = dependency_controller.satisfied(parent_instance.parameters.task.id) %}
    {% if parent_instance.get_output_names(", ") %}
    parent_result = task_futures['{{parent}}'].result()
    {%- for port_name,out in zip(parent_instance.parameters.task.port_names, parent_instance.get_output_names(',').split(','))%}
    {{out}}, pr_{{out}} = (parent_result['{{port_name}}']['output'], parent_result['{{port_name}}']['sample']){% endfor %}{% endif %}
    ts_{{parent_instance.output}} = parent_result['time']
    {%- endif %}
    {%- endfor %}
    
    spark_logging(spark_session).info(
        'Parents completed, submitting {{instance.parameters.task.id}}')

    {%- if not plain %}
    emit_event(name='update task', message='Task running',
               status='RUNNING',
               identifier='{{instance.parameters.task.id}}')
    {%- endif %}
    spark_logging(spark_session).info(
        'Lemonade task {{instance.parameters.task.id}} started') 
  
    start = timer()
    {{instance.generate_code().strip() | indent(width=4, indentfirst=False)}}
    {%- set outputs = instance.get_data_out_names('|').split('|') %}
    {%- if instance.parameters.get('logging', {}).get('log_level') == 'DEBUG' %}
    {%- if outputs %}
    {%- for variable in outputs %}
    juicer_debug(spark_session, '{{instance.__class__.__name__}}', '{{variable}}', {{variable}})
    {%- endfor %}
    {%- endif %}
    {%- endif %}

    {%- if not plain %}
    {%- for gen_result in instance.get_generated_results() %}
    emit_event(name='task result', message='{{gen_result.type}}',
               status='COMPLETED',
               identifier='{{instance.parameters.task.operation.id}}/{{instance.parameters.task.id}}')
    {%- endfor %}
    {%- endif %}

    {%- if instance.parameters.task.forms.get('display_sample', {}).get('value') in (1, '1') %}
    # Outputs' samples
    {%- for out in instance.get_data_out_names(',').split(',') %}
    {%- if out %}
    dataframe_util.emit_sample('{{instance.parameters.task.id}}', {{out}}, emit_event, '{{out}}')
    {%- endif %}
    {%- endfor %}
    {%- endif %}

    {%- if instance.parameters.task.forms.get('display_schema', {}).get('value') in (1, '1') %}
    # Outputs' schemas
    {%- for out in instance.get_data_out_names(',').split(',') %}
    {%- if out %}
    dataframe_util.emit_schema('{{instance.parameters.task.id}}', {{out}}, emit_event, '{{out}}')
    {%- endif %}
    {%- endfor %}
    {%- endif %}

    results = {
      {%- set is_leaf = instance.out_degree == 0 %}
      {%- for port_name,out in zip(instance.parameters.task.port_names, instance.get_output_names(',').split(',')) %}
        {%- if port_name and out %}
         '{{port_name}}': {'output': {{out}}, 'sample': {%- if is_leaf %} take_sample({{out}}) {%- else %} [] {%- endif %}},
        {%- endif %}
      {%- endfor %}
    }
    {%- if not plain %}
    emit_event(name='update task', message='Task completed',
               status='COMPLETED',
               identifier='{{instance.parameters.task.id}}')
    {%- endif %}

    spark_logging(spark_session).info(
        'Lemonade task {{instance.parameters.task.id}} completed')
    time_elapsed = timer() - start
    results['time'] = time_elapsed
    return results

{%- endif %}
{%- endhandleinstance %}
{% endfor %}

def main(spark_session, cached_state, emit_event):
    """ Run generated code """

    try:
        start = time.time()

        session_start_time = time.time()

        {%- for instance in instances %}
        {%- if instance.has_code %}
        {%- if instance.multiple_inputs %}
        {{instance.get_inputs_names.replace(',', '=') }} = None
        {% endif %}
        {%- endif %}
        {%- endfor %}

        {%- for instance in instances %}
        {%- set is_satisfied = dependency_controller.is_satisfied(instance.parameters.task.id) %}
        {%- if instance.has_code and is_satisfied and instance.out_degree == 0 %}
        {%- set s = dependency_controller.satisfied(instance.parameters.task.id) %}
        task_futures['{{instance.parameters.task.id}}'] = executor.submit(
            lambda: {{instance.parameters.task.operation.slug.replace('-', '_')}}_{{instance.order}}(spark_session, cached_state, emit_event))
        {%- endif %}
        {%- endfor %}

        {%- for instance in instances %}
        {%- set is_satisfied = dependency_controller.is_satisfied(instance.parameters.task.id) %}
        {%- if instance.has_code and is_satisfied and instance.out_degree == 0 %}
        {%- set s = dependency_controller.satisfied(instance.parameters.task.id) %}
        task_futures['{{instance.parameters.task.id}}'].result()
        {%- endif %}
        {%- endfor %}

        end = time.time()
        print("{}\t{}".format(end - start, end - session_start_time))
        return {
            'status': 'OK',
            'message': 'Execution defined',
            {%- for instance in instances %}
              {%- set is_satisfied = dependency_controller.is_satisfied(instance.parameters.task.id) %}
              {%- if instance.has_code and is_satisfied %}
                '{{instance.parameters.task.id}}':
                    task_futures['{{instance.parameters.task.id}}'].result(),
              {%- endif %}
            {%- endfor %}
        }
    except Exception, e:
        nfe = 'java.lang.NumberFormatException'
        # noinspection PyUnresolvedReferences
        if hasattr(e, 'java_exception'):
            cause = e.java_exception.getCause()
            value_expr = re.compile(r'.+"(.+)"')
            if cause is not None:
                if cause.getClass().getName() == nfe:
                    value = value_expr.findall(cause.getMessage())[0]
                    raise ValueError('Invalid numeric data in at least one '
                        'data source (attribute: {})'.format(value))
                else:
                    raise
            else:
                raise
        else:
            raise

{%- if execute_main %}

def dummy_emit_event(room, namespace):
    def _dummy_emit_event(name, message, status, identifier, **kwargs):
        return None
    return _dummy_emit_event

from pyspark.sql import SparkSession
spark_session = SparkSession.builder.getOrCreate()
spark_session.sparkContext.setLogLevel('INFO')
main(spark_session, {}, dummy_emit_event(room=-1, namespace='/none'))

{%- endif %}
{#% endautopep8 %#}
