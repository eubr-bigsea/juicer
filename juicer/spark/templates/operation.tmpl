#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Auto-generated Spark code from Lemonade Workflow
(c) Speed Labs - Departamento de Ciência da Computação
    Universidade Federal de Minas Gerais
More information about Lemonade to be provided
"""
from concurrent.futures import ThreadPoolExecutor
import collections
import itertools
import json
import os
import re
import string
import sys
import time
import threading
import traceback
import unicodedata
import juicer.spark.ext as juicer_ext

from textwrap import dedent
from timeit import default_timer as timer

from pyspark.ml import classification, evaluation, feature, tuning, clustering
from pyspark.sql import functions, types, Row, DataFrame
from pyspark.sql.utils import IllegalArgumentException
from pyspark.sql.window import Window
from pyspark.ml.linalg import Vectors, VectorUDT

from pyspark.ml import Pipeline
from pyspark.ml.classification import *
from pyspark.ml.clustering import *
from pyspark.ml.evaluation import *
from pyspark.ml.feature import *
from pyspark.ml.tuning import *
from pyspark.ml.recommendation import *
from pyspark.ml.regression import *
from pyspark.mllib.evaluation import *
from juicer import privaaas
from juicer.util import dataframe_util
from juicer.spark.reports import *

executor = ThreadPoolExecutor(max_workers={{instances|length}})
submission_lock = threading.Lock()
task_futures = {}

# We need the following code in order to integrate this application log with the
# spark standard log4j. Thus, we set *__SPARK_LOG__* at most one time per
# execution.
__SPARK_LOG__ = None


def spark_logging(spark_session):
    global __SPARK_LOG__
    if not __SPARK_LOG__:
        # noinspection PyProtectedMember
        logger = spark_session.sparkContext._jvm.org.apache.log4j
        __SPARK_LOG__ = logger.LogManager.getLogger(__name__)
    return __SPARK_LOG__



def take_sample(df, size=100, default=None):
    """
    Takes a sample from data frame.
    """
    result = default or []
    if hasattr(df, 'take'):
        # result = df.limit(size).toJSON().collect()
        header = ','.join([f.name for f in df.schema.fields])
        result = [header]
        result.extend(
            df.limit(size).rdd.map(dataframe_util.convert_to_csv).collect())
    return result


def juicer_debug(spark_session, name, variable, data_frame):
    """ Debug code """
    spark_logging(spark_session).debug('#' * 20)
    spark_logging(spark_session).debug('|| {} ||'.format(name))
    spark_logging(spark_session).debug('== {} =='.format(variable))
    data_frame.show()
    schema = data_frame.schema
    for attr in schema:
        spark_logging(spark_session).debug('{} {} {} {}'.format(
            attr.name, attr.dataType, attr.nullable, attr.metadata))
{#% autopep8 %#}
{%- for instance in instances %}
{%-  handleinstance instance %}
{%- if instance.has_code %}

# noinspection PyUnusedLocal
def {{instance.parameters.task.operation.slug.replace('-', '_')}}_{{instance.order}}(spark_session, cached_state, emit_event):
    """
    {%- if instance.parameters.task.forms.comment and instance.parameters.task.forms.comment.value %}
    {{instance.parameters.task.forms.comment.value.strip().replace('"', '')}}
    {%- else %}
    Operation {{instance.parameters.task.id }}
    {%- endif %}
    Task hash: {{instance.parameters.hash}}.
    """
    {%- if instance.parameters.task.parents %}
    spark_logging(spark_session).info(
        "Submitting parent task(s)"
        "{{instance.parameters.task.parents|join(', ')}}"
        "before {{instance.parameters.task.id}}")
    # If the task's result is not cached, we submit its dependencies first
    {%- for parent in instance.parameters.task.parents %}
    {%- set parent_instance = instances_by_task_id[parent] %}
    {%- set is_satisfied = dependency_controller.is_satisfied(parent_instance.parameters.task.id) %}
    {%- if parent_instance.has_code and is_satisfied %}
    {%- set s = dependency_controller.satisfied(parent_instance.parameters.task.id) %}
    with submission_lock:
        if '{{parent}}' not in task_futures:
            task_futures['{{parent}}'] = executor.\
                submit(
                    lambda: {{parent_instance.parameters.task.operation.slug.replace('-', '_')}}_{{parent_instance.order}}(
                        spark_session, cached_state, emit_event)
                )

    {%- endif %}
    {%- endfor %}
    {%- endif %}

    # Next we wait for the dependencies to complete
    {%- for parent in instance.parameters.task.parents %}
    {%- set parent_instance = instances_by_task_id[parent] %}
    {%- set is_satisfied = dependency_controller.is_satisfied(parent_instance.parameters.task.id) %}
    {%- if parent_instance.has_code and is_satisfied %}
    {%- set s = dependency_controller.satisfied(parent_instance.parameters.task.id) %}
    {% if parent_instance.get_output_names(", ") %}
    parent_result = task_futures['{{parent}}'].result()
    {%- for port_name,out in zip(parent_instance.parameters.task.port_names, parent_instance.get_output_names(',').split(','))%}
    {{out}}, pr_{{out}} = (parent_result['{{port_name}}']['output'],
        parent_result['{{port_name}}']['sample'])
    {%- endfor %}
    {%- endif %}
    ts_{{parent_instance.output}} = parent_result['time']
    {%- endif %}
    {%- endfor %}
    
    spark_logging(spark_session).info(
        'Parents completed, submitting {{instance.parameters.task.id}}')


    {%- if not plain %}
    emit_event(name='update task', message=_('Task running'),
               status='RUNNING',
               identifier='{{instance.parameters.task.id}}')
    {%- endif %}
    spark_logging(spark_session).info(
        'Lemonade task {{instance.parameters.task.id}} started')

    start = timer()

    # First we verify whether this task's result is cached
    supports_cache = {{instance.supports_cache}}
    results = None

    if supports_cache and '{{instance.parameters.task.id}}' in cached_state:
        cached, _hash = cached_state.get('{{instance.parameters.task.id}}')
        if _hash == '{{instance.parameters.hash}}':
            emit_event(name='update task', message=_('Task running (cached data)'),
                       status='RUNNING',
                       identifier='{{instance.parameters.task.id}}')
            spark_logging(spark_session).info (
                'Cache hit for operation {}'.format(
                '{{instance.parameters.task.id}}'))
            results = cached
    if results is None:
        {{instance.generate_code().strip() | indent(width=8, indentfirst=False)}}
        {%- set outputs = instance.get_data_out_names('|').split('|') %}
        {%- if instance.parameters.get('logging', {}).get('log_level') == 'DEBUG' %}
        {%- if outputs %}
        {%- for variable in outputs %}
        juicer_debug(spark_session, '{{instance.__class__.__name__}}', '{{variable}}', {{variable}})
        {%- endfor %}
        {%- endif %}
        {%- endif %}

        {%- if not plain %}
        {%- for gen_result in instance.get_generated_results() %}
        emit_event(name='task result', message=_('{{gen_result.type}}'),
                   status='COMPLETED',
                   identifier='{{instance.parameters.task.operation.id}}/{{instance.parameters.task.id}}')
        {%- endfor %}
        {%- endif %}

        {#-
            Privacy related part.
        #}
        {%- set traces = instance.attribute_traceability() %}
        {%- if traces %}
        if dataframe_util.spark_version(spark_session) >= (2, 2, 0):
        {%- for trace in traces %}
        {%- for out in instance.get_data_out_names(',').split(',') %}
        {%- if out %}
            schema = {{trace.input}}.schema
            meta = {}
            {%- if trace.derived_from != '*' %}
            meta.update(schema['{{trace.derived_from}}'].metadata)
            {%- endif %}
            {{out}} = {{out}}.withColumn('{{trace.attribute}}',
                functions.col('{{trace.attribute}}').alias('', metadata=meta))
        {%- endif %}
        {%- endfor %}
        {%- endfor %}
            pass
        {%- endif %}

        results = {
          {%- set is_leaf = instance.out_degree == 0 %}
          {%- for port_name,out in zip(instance.parameters.task.port_names, instance.get_output_names(',').split(',')) %}
            {%- if port_name and out %}
             '{{port_name}}': {'output': {{out}}, 'sample': {%- if is_leaf %} take_sample({{out}}) {%- else %} [] {%- endif %}},
            {%- endif %}
          {%- endfor %}
        }

    df_types = (DataFrame, dataframe_util.LazySparkTransformationDataframe)
    {%- if instance.has_code and instance.parameters.task.forms.get('display_sample', {}).get('value') in (1, '1') %}
    for name, out in results.items():
        if (isinstance(out, dict) and 'output' in out
                and isinstance(out['output'], df_types)):
            dataframe_util.emit_sample(
                '{{instance.parameters.task.id}}', out['output'], emit_event, name)
    {%- endif %}
    {%- if instance.has_code and instance.parameters.task.forms.get('display_schema', {}).get('value') in (1, '1') %}
    for name, out in results.items():
        if (isinstance(out, dict) and 'output' in out
                and isinstance(out['output'], df_types)):
            dataframe_util.emit_schema(
                '{{instance.parameters.task.id}}', out['output'], emit_event, name)
    {%- endif %}

    {%- if not plain %}
    emit_event(name='update task', message=_('Task completed'),
               status='COMPLETED',
               identifier='{{instance.parameters.task.id}}')
    {%- endif %}

    spark_logging(spark_session).info(
        'Lemonade task {{instance.parameters.task.id}} completed')
    time_elapsed = timer() - start
    results['time'] = time_elapsed
    return results

{%- endif %}
{%- endhandleinstance %}
{% endfor %}

def get_results(task_futures, id):
    return task_futures[id].result() if id in task_futures else None

def main(spark_session, cached_state, emit_event):
    """ Run generated code """

    try:
        start = time.time()

        session_start_time = time.time()

        {%- for instance in instances %}
        {%- if instance.has_code %}
        {%- if instance.multiple_inputs %}
        {{instance.get_inputs_names.replace(',', '=') }} = None
        {% endif %}
        {%- endif %}
        {%- endfor %}

        {%- for instance in instances %}
        {%- set is_satisfied = dependency_controller.is_satisfied(instance.parameters.task.id) %}
        {%- if instance.must_be_executed(is_satisfied) %}
        {%- set s = dependency_controller.satisfied(instance.parameters.task.id) %}
        task_futures['{{instance.parameters.task.id}}'] = executor.submit(
            lambda: {{instance.parameters.task.operation.slug.replace('-', '_')}}_{{instance.order}}(spark_session, cached_state, emit_event))
        {%- endif %}
        {%- endfor %}

        {%- for instance in instances %}
        {%- set is_satisfied = dependency_controller.is_satisfied(instance.parameters.task.id) %}
        {%- if instance.must_be_executed(is_satisfied) %}
        {%- set s = dependency_controller.satisfied(instance.parameters.task.id) %}
        task_futures['{{instance.parameters.task.id}}'].result()
        {%- endif %}
        {%- endfor %}

        end = time.time()
        print(end - start)
        return {
            'status': 'OK',
            'message': 'Execution defined',
            'hashes': {
                {%- for instance in instances %}
                    '{{instance.parameters.task.id}}': '{{instance.parameters.hash}}',
                {%-endfor %}
            },
            {%- for instance in instances %}
              {%- set is_satisfied = dependency_controller.is_satisfied(instance.parameters.task.id) %}
              {%- if instance.has_code and is_satisfied %}
                '{{instance.parameters.task.id}}':
                    [get_results(task_futures, '{{instance.parameters.task.id}}'),
                    '{{instance.parameters.hash}}'],
              {%- endif %}
            {%- endfor %}
        }
    except Exception as e:
        spark_session.sparkContext.cancelAllJobs()
        traceback.print_exc(file=sys.stderr)
        if not dataframe_util.handle_spark_exception(e):
            raise

{%- if execute_main %}

def dummy_emit_event(room, namespace):
    def _dummy_emit_event(name, message, status, identifier, **kwargs):
        return None
    return _dummy_emit_event

from pyspark.sql import SparkSession
spark_session = SparkSession.builder.getOrCreate()
spark_session.sparkContext.setLogLevel('INFO')
main(spark_session, {}, dummy_emit_event(room=-1, namespace='/none'))

{%- endif %}
{#% endautopep8 %#}
