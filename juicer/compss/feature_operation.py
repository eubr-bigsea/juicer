# -*- coding: utf-8 -*-

from textwrap import dedent
from juicer.operation import Operation
try:
    from itertools import zip_longest as zip_longest
except ImportError:
    from itertools import zip_longest as zip_longest


class FeatureAssemblerOperation(Operation):
    """FeatureAssemblerOperation.

    Feature Assembler is a transformer that combines a given list of columns
    into a single vector column. It is useful for combining raw features and
    features generated by different feature transformers into a single feature
    vector, in order to train ML models.

    Feature Assembler accepts the following input column types: all numeric
    types, boolean type, and vector type. In each row, the values of the
    input columns will be concatenated into a vector in the specified order.
    """

    def __init__(self, parameters,  named_inputs, named_outputs):
        Operation.__init__(self, parameters,  named_inputs,  named_outputs)

        if 'attributes' not in parameters:
            raise ValueError(
                _("Parameters '{}' must be informed for task {}")
                .format('attributes', self.__class__))

        tmp = 'output_data_{}'.format(self.order)
        self.alias = parameters.get("alias", 'FeatureField')
        self.output = self.named_outputs.get('output data', tmp)

        self.has_code = len(self.named_inputs) == 1
        if self.has_code:
            self.has_import = "from functions.ml.feature_assembler " \
                              "import FeatureAssemblerOperation\n"

    def get_optimization_information(self):
        # optimization problemn: iteration over others fragments
        flags = {'one_stage': True,  # if has only one stage
                 'keep_balance': True,  # if number of rows doesnt change
                 'bifurcation': False,  # if has two inputs
                 'if_first': False,  # if need to be executed as a first task
                 }
        return flags

    def generate_preoptimization_code(self):
        """Generate code for optimization task."""
        code = """
        settings = dict()
        settings['cols'] = {cols}
        settings['alias'] = '{alias}'
        conf.append(FeatureAssemblerOperation().preprocessing(settings))
        """.format(alias=self.alias, cols=self.parameters['attributes'])
        return code

    def generate_optimization_code(self):
        """Generate code for optimization task."""
        code = """
        {output} = FeatureAssemblerOperation()
                        .transform_serial({input}, conf_X)
        """.format(output=self.output,
                   input=self.named_inputs['input data'])
        return dedent(code)

    def generate_code(self):
        """Generate code."""
        code = """
            settings = dict()
            settings['cols'] = {cols}
            settings['alias'] = '{alias}'
            {output} = FeatureAssemblerOperation()\
                            .transform({input}, settings, numFrag)
            """.format(output=self.output, alias=self.alias,
                       input=self.named_inputs['input data'],
                       cols=self.parameters['attributes'])

        return dedent(code)


class StringIndexerOperation(Operation):
    """StringIndexerOperation.

    Indexes a feature by encoding a string column as
    a column containing indexes.
    """

    def __init__(self, parameters,  named_inputs, named_outputs):
        Operation.__init__(self, parameters,  named_inputs,  named_outputs)

        if 'attributes' not in parameters:
            raise ValueError(
                _("Parameters '{}' must be informed for task {}")
                .format('attributes', self.__class__))
        elif 'alias' not in parameters:
            raise ValueError(
                _("Parameters '{}' must be informed for task {}")
                .format('alias', self.__class__))

        self.output = self.named_outputs.get('output data',
                                             'output_data_{}'.format(
                                                     self.order))
        self.alias = parameters.get("alias", 'FeatureIndexed')
        self.has_code = len(self.named_inputs) == 1
        if self.has_code:
            self.has_import = "from functions.ml.string_indexer " \
                              "import StringIndexerOperation\n"

    def get_optimization_information(self):
        # optimization problemn: iteration over others fragments
        flags = {'one_stage': False,  # if has only one stage
                 'keep_balance': True,  # if number of rows doesnt change
                 'bifurcation': False,  # if has two inputs
                 'if_first': True,  # if need to be executed as a first task
                 }

        return flags

    def generate_preoptimization_code(self):
        """Generate code for optimization task."""
        code = """
        settings = dict()
        settings['inputCol'] = {columns}
        settings['outputCol'] = '{alias}'
        conf.append(StringIndexerOperation().preprocessing(settings,
        {input}, numFrag))
        """.format(alias=self.alias, input=self.named_inputs['input data'],
                   columns=self.parameters['attributes'])
        return code

    def generate_optimization_code(self):
        """Generate code for optimization task."""
        code = """
        {output} = StringIndexerOperation().transform_serial({input},
        conf_X[0], conf_X[1], conf_X[2])
        """.format(output=self.output,
                   input=self.named_inputs['input data'])
        return dedent(code)

    def generate_code(self):
        """Generate Code."""
        code = """
            settings = dict()
            settings['inputCol'] = {columns}
            settings['outputCol'] = '{alias}'
            {out}, mapper = StringIndexerOperation()\
                .transform({input}, settings, numFrag)
            """.format(out=self.output, alias=self.alias,
                       input=self.named_inputs['input data'],
                       columns=self.parameters['attributes'],
                       mode=self.mode)

        return dedent(code)


class PCAOperation(Operation):
    """PCA Operation

    Principal component analysis (PCA) is a statistical method to find
    a rotation such that the first coordinate has the largest variance
    possible, and each succeeding coordinate in turn has the largest
    variance possible. The columns of the rotation matrix are called
    principal components. PCA is used widely in dimensionality reduction.
    """

    def __init__(self, parameters,  named_inputs, named_outputs):
        Operation.__init__(self, parameters,  named_inputs,  named_outputs)

        if 'attributes' not in parameters:
            raise ValueError(
                _("Parameters '{}' must be informed for task {}")
                .format('attributes', self.__class__))

        tmp = 'output_data_{}'.format(self.order)
        self.alias = parameters.get("alias", 'FeatureField')
        self.output = self.named_outputs.get('output data', tmp)

        self.n_components = 1

        self.has_code = len(self.named_inputs) == 1
        if self.has_code:
            self.has_import = "from functions.ml.pca import PCAOperation\n"

    def get_optimization_information(self):
        # optimization problemn: iteration over others fragments
        flags = {'one_stage': False,  # if has only one stage
                 'keep_balance': True,  # if number of rows doesnt change
                 'bifurcation': False,  # if has two inputs
                 'if_first': True,  # if need to be executed as a first task
                 }
        return flags

    def generate_preoptimization_code(self):
        """Generate code for optimization task."""
        code = """
        settings = dict()
        settings['features'] = '{col}'
        settings['predCol'] = '{alias}'
        settings['NComponents'] = {n_comp}
        conf.append(PCAOperation().fit({input}, settings, numFrag))
        """.format(alias=self.alias, col=self.parameters['attributes'][0],
                   n_comp=self.n_components,
                   input=self.named_inputs['input data'])
        return code

    def generate_optimization_code(self):
        """Generate code for optimization task."""
        code = """
        {output} = PCAOperation().transform_serial({input}, conf_X)
        """.format(output=self.output,
                   input=self.named_inputs['input data'])
        return dedent(code)

    def generate_code(self):
        """Generate code."""
        code = """
            settings = dict()
            settings['features'] = '{col}'
            settings['predCol'] = '{alias}'
            settings['NComponents'] = {n_comp}
            pca = PCAOperation()
            model = pca.fit(settings)
            {output} = pca.transform({input}, model, settings, numFrag)
            """.format(alias=self.alias,
                       col=self.parameters['attributes'][0],
                       n_comp=self.n_components, output=self.output,
                       input=self.named_inputs['input data'])

        return dedent(code)


class MaxAbsScalerOperation(Operation):
    def __init__(self, parameters,  named_inputs, named_outputs):
        Operation.__init__(self, parameters,  named_inputs,  named_outputs)

        if 'attribute' not in parameters:
            raise ValueError(
                _("Parameter '{}' must be informed for task {}")
                .format('attributes', self.__class__))

        tmp = 'output_data_{}'.format(self.order)
        self.output = self.named_outputs.get('output data', tmp)
        self.alias = [s.strip()
                      for s in parameters.get("alias", []).split(',')]
        self.attributes = parameters['attribute']
        # Adjust alias in order to have the same number of aliases
        # as attributes by filling missing alias with the attribute
        # name sufixed by _indexed.
        if len(self.alias) > 0:
            self.alias = [x[1] or '{}_norm'.format(x[0]) for x in
                          zip_longest(self.attributes,
                                       self.alias[:len(self.attributes)])]

        self.has_code = len(self.named_inputs) == 1
        if self.has_code:
            self.has_import = "from functions.ml.maxabs_scaler.py " \
                              "import MaxAbsScalerOperation\n"

    def get_optimization_information(self):
        flags = {'one_stage': False,  # if has only one stage
                 'keep_balance': True,  # if number of rows doesnt change
                 'bifurcation': False,  # if has two inputs
                 'if_first': True,  # if need to be executed as a first task
                 }
        return flags

    def generate_preoptimization_code(self):
        """Generate code for optimization task."""
        code = """
        settings = dict()
        settings['attributes'] = {att}
        settings['alias'] = {alias}
        model = MaxAbsScalerOperation().fit({input}, settings, numFrag)
        conf.append([settings, model])
        """.format(input=self.named_inputs['input data'],
                   att=self.attributes, alias=self.alias)
        return code

    def generate_optimization_code(self):
        """Generate code for optimization task."""
        code = """
        confs, model = conf_X
        {output} = MaxAbsScalerOperation().transform_serial(({input}, model, confs, idfrag)
        """.format(output=self.output, input=self.named_inputs['input data'])
        return dedent(code)

    def generate_code(self):
        """Generate code."""
        code = """
        settings = dict()
        settings['attributes'] = {att}
        settings['alias'] = {alias}
        model = MaxAbsScalerOperation().fit({input}, settings, numFrag)
        {output} = MaxAbsScalerOperation().transform({input}, model, settings, numFrag)
        """.format(output=self.output, input=self.named_inputs['input data'],
                   att=self.attributes, alias=self.alias)
        return dedent(code)


class MinMaxScalerOperation(Operation):
    def __init__(self, parameters,  named_inputs, named_outputs):
        Operation.__init__(self, parameters,  named_inputs,  named_outputs)

        if 'attribute' not in parameters:
            raise ValueError(
                _("Parameters '{}' must be informed for task {}")
                .format('attribute', self.__class__))

        tmp = 'output_data_{}'.format(self.order)
        self.output = self.named_outputs.get('output data', tmp)
        self.alias = [s.strip()
                      for s in parameters.get("alias", []).split(',')]
        self.attributes = parameters['attribute']
        # Adjust alias in order to have the same number of aliases
        # as attributes by filling missing alias with the attribute
        # name sufixed by _indexed.
        if len(self.alias) > 0:
            self.alias = [x[1] or '{}_norm'.format(x[0]) for x in
                          zip_longest(self.attributes,
                                       self.alias[:len(self.attributes)])]

        self.min = parameters.get('min', 0)
        self.max = parameters.get('max', 1)
        self.has_code = len(self.named_inputs) == 1
        if self.has_code:
            self.has_import = "from functions.ml.minmas_scaler.py " \
                              "import MinMaxScalerOperation\n"

    def get_optimization_information(self):
        flags = {'one_stage': False,  # if has only one stage
                 'keep_balance': True,  # if number of rows doesnt change
                 'bifurcation': False,  # if has two inputs
                 'if_first': True,  # if need to be executed as a first task
                 }
        return flags

    def generate_preoptimization_code(self):
        """Generate code for optimization task."""
        code = """
        settings = dict()
        settings['attributes'] = {att}
        settings['alias'] = {alias}
        settings['min'] = {min}
        settings['max'] = {max}
        model = MinMaxScalerOperation().fit({input}, settings, numFrag)
        conf.append([model, settings])
        """.format(input=self.named_inputs['input data'], att=self.attributes,
                   alias=self.alias, min=self.min, max=self.max)
        return code

    def generate_optimization_code(self):
        """Generate code for optimization task."""
        code = """
        confs, model = conf_X
        {output} = MinMaxScalerOperation().transform_serial(({input}, model, confs, idfrag)
         """.format(output=self.output, input=self.named_inputs['input data'])
        return dedent(code)

    def generate_code(self):
        """Generate code."""
        code = """
        settings = dict()
        settings['attributes'] = {att}
        settings['alias'] = {alias}
        settings['min'] = {min}
        settings['max'] = {max}
        model = MinMaxScalerOperation().preprocessing({input}, settings, numFrag)
        {output} = MinMaxScalerOperation().transform({input}, model, settings, numFrag)
        """.format(output=self.output, input=self.named_inputs['input data'],
                   att=self.attributes, alias=self.alias, min=self.min,
                   max=self.max)
        return dedent(code)


class StandardScalerOperation(Operation):
    def __init__(self, parameters,  named_inputs, named_outputs):
        Operation.__init__(self, parameters,  named_inputs,  named_outputs)

        if 'attribute' not in parameters:
            raise ValueError(
                _("Parameters '{}' must be informed for task {}")
                .format('attribute', self.__class__))

        tmp = 'output_data_{}'.format(self.order)
        self.output = self.named_outputs.get('output data', tmp)
        self.alias = [s.strip()
                      for s in parameters.get("alias", []).split(',')]
        self.attributes = parameters['attribute']
        # Adjust alias in order to have the same number of aliases
        # as attributes by filling missing alias with the attribute
        # name sufixed by _indexed.
        if len(self.alias) > 0:
            self.alias = [x[1] or '{}_norm'.format(x[0]) for x in
                          zip_longest(self.attributes,
                                       self.alias[:len(self.attributes)])]

        self.has_code = len(self.named_inputs) == 1
        if self.has_code:
            self.has_import = "from functions.ml.standard_scaler.py " \
                              "import StandardScalerOperation\n"

    def get_optimization_information(self):
        flags = {'one_stage': False,  # if has only one stage
                 'keep_balance': True,  # if number of rows doesnt change
                 'bifurcation': False,  # if has two inputs
                 'if_first': True,  # if need to be executed as a first task
                 }
        return flags

    def generate_preoptimization_code(self):
        """Generate code for optimization task."""
        code = """
        settings = dict()
        settings['attributes'] = {att}
        settings['alias'] = {alias}
        model = StandardScalerOperation().preprocessing({input}, settings, numFrag)
        conf.append([model, settings])
        """.format(input=self.named_inputs['input data'],
                   att=self.attributes, alias=self.alias)
        return code

    def generate_optimization_code(self):
        """Generate code for optimization task."""
        code = """
        confs, model = conf_X
        {output} = StandardScalerOperation().transform_serial(({input}, model, confs, idfrag)
        """.format(output=self.output, input=self.named_inputs['input data'])
        return dedent(code)

    def generate_code(self):
        """Generate code."""
        code = """
        settings = dict()
        settings['attributes'] = {att}
        settings['alias'] = {alias}
        model = StandardScalerOperation().preprocessing({input}, settings, numFrag)
        {output} = StandardScalerOperation().transform({input}, model, settings, numFrag)
        """.format(output=self.output, input=self.named_inputs['input data'],
                   att=self.attributes, alias=self.alias)
        return dedent(code)
