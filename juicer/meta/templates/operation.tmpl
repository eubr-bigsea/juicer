#!/usr/bin/env python/
"""
Auto-generated Spark code from Lemonade Workflow
(c) Speed Labs - Departamento de Ciência da Computação
    Universidade Federal de Minas Gerais
More information about Lemonade to be provided.
"""
from seaborn import jointplot
from pyspark.ml import (classification, regression, clustering,
                        evaluation, feature, tuning, Pipeline, PipelineModel)
from pyspark.sql import functions, types, Row, DataFrame

from pyspark.mllib.evaluation import MulticlassMetrics

from collections import namedtuple
from concurrent.futures import ThreadPoolExecutor, as_completed
import logging
import logging.config
import functools
import itertools
import math
import operator
import random
import traceback
import sys
import time
import threading
import uuid
import numpy as np
import os
from juicer.util import dataframe_util
from juicer.spark.reports import (AreaUnderCurveReport,
                                  ConfusionMatrixImageReport,
                                  MatplotlibChartReport, PlotlyChartReport,
                                  SeabornChartReport)
from juicer.spark.util.results import CurveMetrics
import dataclasses

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

{% set builder_params = transpiler.transpiler.prepare_model_builder_parameters(instances) %}
{%- set task_type = builder_params.evaluator.task_type %}
{% for imps in transpiler.imports %}
# FIXME: Do not work
{{imps}}
{%- endfor %}

submission_lock = threading.Lock()

@dataclasses.dataclass(frozen=True)
class TrainingContext:
    index: int
    task_id: int
    task_name: str
    name: str
    details: str
    operation_id: int
    start: float
    time: float
    fold: int
    def replace(self, *args, start: float, time: float):
       return dataclasses.replace(
           self, start=start, time=time)

@dataclasses.dataclass(frozen=True)
class TrainingResult:
    index: int = None
    status: str = None
    model: str = None
    details: str = None
    metric: str = None
    exception: str = None
    ctx: object = None
    feature_importance: str = None
    fold: int = 0
    def __iter__(self):
        return iter(
            (self.index, self.status, self.model, self.details, self.metric,
            self.exception, self.ctx, self.feature_importance)
        )

{% for auxiliary_code in transpiler.get_auxiliary_code(instances) %}
{%- include auxiliary_code with context %}
{%- endfor %}

def get_feature_importance_score(model, features, classes, data=None):
    if isinstance(model, clustering.KMeansModel):
        result = {
            "training cost": model.summary.trainingCost,
            "cluster sizes": model.summary.clusterSizes,
        }
        return result
    elif isinstance(model, classification.LogisticRegressionModel):
        m = model.extractParamMap()
        if m.get(model.family) in ('multinomial', 'auto'):
            coef = model.coefficientMatrix.toArray()
            coefficients_dict = []
            odds_dict = []
            total_classes = len(classes)
            #odds_ratios = np.exp(coef)
            for i, row in enumerate(coef):
                class_coeff = {
                    "class": classes[i] if i < total_classes else str(i),
                }
                class_odds = {
                    "class": classes[i] if i < total_classes else str(i),
                }

                for j, col in enumerate(row):
                    class_coeff[features[j]] = float(col)
                    #class_odds[features[j]] = float(odds_ratios[i, j])

                coefficients_dict.append(class_coeff)
                #odds_dict.append(class_odds)
            result = {'coefficients': coefficients_dict, }
        else:
            coef = list(model.coefficients.tolist())
            odds_ratios = [math.exp(c) for c in coef]
            result = {'coefficients': dict(zip(features, coef)),
                      'odds_ratios': dict(zip(features, odds_ratios))}
        return result
    elif isinstance(model, classification.LinearSVCModel):
        coefficients = model.coefficients
        return {'coefficients': dict(zip(features, coefficients))}

    elif isinstance(model,
                    (classification.DecisionTreeClassificationModel,
                     classification.RandomForestClassificationModel,
                     classification.GBTClassificationModel,
                     regression.DecisionTreeRegressionModel,
                     regression.GBTRegressionModel,
                     regression.RandomForestRegressionModel)):
        return {'importance': dict(zip(features,
            model.featureImportances.toArray().tolist()))
        }
    elif isinstance(model, (classification.NaiveBayesModel)):
        return {
            'pi': model.pi.values.tolist(),
            'theta': model.theta.values.tolist(),
        }
    elif isinstance(model, (regression.LinearRegressionModel,
            regression.GeneralizedLinearRegressionModel)):
        return {
            'coefficients': dict(zip(features, model.coefficients.tolist())),
            'intercept': model.intercept,
            'importance': dict(zip(features, map(abs, model.coefficients.tolist()))),
        }
    elif isinstance(model, (regression.FMRegressionModel,
                            classification.FMClassificationModel)):
        return {
            'coefficients': dict(zip(features, model.linear.tolist())),
            'intercept': model.intercept,
            'importance': dict(zip(features, map(abs, model.linear.tolist()))),
        }
    else:
        print('>>>>>>>>>>>>>>>>>>', type(model))

class CustomTrainValidationSplit():
    """ Mimic the TrainValidationSplit from Spark, but emit information
    about the training process after each iteration."""
    __slots__ = (
        'estimator', 'evaluator', 'train_ratio', 'seed', 'params', 'train_size',
        'test_size', 'num_workers', 'strategy', 'folds','fold_col')
    def __init__(self, estimator, evaluator, params,
                 train_ratio=0.7, seed=None,
                 num_workers=4, strategy='split', folds=None):

        self.estimator = estimator
        self.evaluator = evaluator
        self.train_ratio = train_ratio
        self.seed = seed
        self.params = params
        self.train_size = 0
        self.test_size = 0
        self.num_workers = num_workers
        self.strategy = strategy
        self.folds = folds
        self.fold_col = None

    def _perform_training(self, train, test, estimator, evaluator, params,
                          details, ctx):
        try:
            start = time.time()
            model = estimator.fit(train, params)
            df = model.transform(test, params)
            metric = evaluator.evaluate(df)
            new_ctx = ctx.replace(start=start, time=time.time() - start)
            return TrainingResult(ctx.index, 'OK', model, details, metric, ctx=new_ctx)
        except Exception as e:
            try:
                dataframe_util.handle_spark_exception(e)
            except ValueError as ve:
                logger.exception(ve)
                return TrainingResult(ctx.index, 'ERROR', model=None, metric=None,
                                      details=details, exception=ve, ctx=ctx)

    def _extract_params(self, params):
        task_id, task_name, operation_id, estimator, details = (
            None, None, None, None, {})
        for param, value in params.items():
            if param.name == 'stages':
                task_id = value[-1].task_id
                task_name = value[-1].task_name
                operation_id = value[-1].operation_id
                estimator = value[-1].__class__.__name__
            else:
                details[param.name] = value
        return task_id, task_name, operation_id, estimator, details

    def _generate_folds(self, df):
        if self.strategy == 'split':
            return [df.randomSplit(
                [self.train_ratio, 1 - self.train_ratio], self.seed)]
        elif self.strategy == 'cross_validation':
            return self._k_fold()
        else:
            raise ValueError('Invalid train split: {}'.format(self.strategy))

    def _k_fold(self, df):
        """ Generates k-fold. Based on the PySpark implementation:
        https://github.com/apache/spark/blob/master/python/pyspark/ml/tuning.py
        """
        datasets = []

        if self.fold_col:
            # FIXME: Implement support to fold_col
            pass
        else:
            h = 1.0 / self.folds
            rand_col = f'{uuid.uuid5()}_rand'
            df = df.select("*", functions.rand(self.seed).alias(rand_col))
            for i in range(self.folds):
                validate_lower_bound = i * h
                validate_upper_bound = (i + 1) * h
                condition = ((df[rand_col] >= validate_lower_bound) &
                             (df[rand_col] < validate_upper_bound))
                validation = df.filter(condition)
                train = df.filter(~condition)
                datasets.append((train, validation))

            return datasets

    def fit(self, df, emit, features_names, label_attr):
        results = []

        with ThreadPoolExecutor(max_workers=self.num_workers) as executor:
            folds = self._generate_folds(df)
            for fold_n, (train, test) in enumerate(folds):
                train.cache()
                test.cache()
                future_train = {}
                for counter, params in enumerate(self.params):
                    task_id, task_name, operation_id, estimator, details = (
                        self._extract_params(params))

                    ctx = TrainingContext(counter, task_id, task_name,
                        estimator, details, operation_id, time.time(),
                        0, fold_n)
                    submission = executor.submit(
                        self._perform_training, train, test, self.estimator,
                        self.evaluator, params, details, ctx)
                    future_train[submission] = ctx

                is_larger_better = self.evaluator.isLargerBetter()
                for future in as_completed(future_train):
                    result = future.result()
                    # future_train[future]
                    if result is None:
                        log.warning('Result is None')
                        continue
                    ctx = result.ctx
                    message = {'params': ctx.details, 't': ctx.time,
                        'index': ctx.index}
                    if result.metric is not None and math.isnan(result.metric):
                         message['error'] = 'Metric could not be computed'
                         emit(identifier=ctx.task_id, status='ERROR',
                                message=message, title=ctx.task_name,
                                operation_id=ctx.operation_id)

                    elif result.status == 'OK':
                        results.append(result)
                        metric_name = self.evaluator.extractParamMap().get(
                            self.evaluator.metricName)
                        message['metric'] = {
                            'name': metric_name,
                            'value': result.metric if not math.isnan(result.metric) else 1000,
                            'is_larger_better': is_larger_better,
                            'index': ctx.index, 'fold': ctx.fold
                        }
                        ml_model = result.model.stages[-1]

                        # Test if label was indexed with StringIndexer
                        stage_indexer_label = next(
                            (s for s in result.model.stages
                                if isinstance(s, feature.StringIndexerModel)
                                and s.getInputCol() == label_attr), None
                        )
                        if stage_indexer_label is not None:
                            classes = stage_indexer_label.labels
                        else:
                            classes = []
                        importance = get_feature_importance_score(
                            ml_model, features_names, classes, df)
                        message['feature_importance'] = importance
                        emit(identifier=ctx.task_id, status='COMPLETED',
                            message=message, title=ctx.task_name,
                            operation_id=ctx.operation_id, )
                    else:
                        message['error'] = str(result.exception)
                        emit(identifier=ctx.task_id, status='ERROR',
                                message=message, title=ctx.task_name,
                                operation_id=ctx.operation_id)


                self.train_size = train.count()
                self.test_size = test.count()
                # Unpersist training & validation set once all metrics have been produced
                train.unpersist()
                test.unpersist()

        return results


def emit_metric(emit_event):
    return functools.partial(
        emit_event, name='task result', type='METRIC')

def read_data(spark_session):
    """ Read input data."""
    {%- if builder_params.read_data %}

    {{builder_params.read_data.model_builder_code() | indent(width=4, first=False)}}
    {%- endif %}

    # Many Spark 2.x transformers, including Binarizer and Imputer,
    # work only with float or double types.
    # Decimal and even integer is not supported.
    # See https://issues.apache.org/jira/browse/SPARK-20604.
    # Current solution is to convert all unsupported numeric data types
    # to double
    for s in df.schema:
        if s.dataType.typeName() in ['integer', 'decimal']:
            df = df.withColumn(s.name, df[s.name].cast('double'))

    {% for feat in builder_params.features.numerical_features %}
    {%- if feat.feature_type == 'numerical' %}
    if df.schema['{{feat.name}}'].dataType.typeName() == 'string':
        # Cast features marked as Numeric, but loaded as String
        df = df.withColumn('{{feat.name}}', df['{{feat.name}}'].cast('double'))
    {%- endif %}
    {%- endfor %}


    return df

def create_pipeline(label: str, features_names):
    """ Define a Spark pipeline model."""

    # Feature engineering
    features_stages = []

    {{builder_params.features.generate_code() | indent(width=4, first=False)}}
    {%- set has_numerical_features = builder_params.features.numerical_features|length > 0%}
    {%- set use_pca = builder_params.reduction.has_code and has_numerical_features and has_numerical_features%}

    {%- if use_pca %}

    # Feature reduction
    numerical_features = [
        {%- for f in builder_params.features.numerical_features %}
        '{{f.name}}'{% if not loop.last %}, {% endif %}
        {%- endfor %}
    ]
    {{builder_params.reduction.generate_code() | indent(width=4, first=False)}}
    {% else %}
    {%- endif %}

    # Assembling of all features
    vec_assembler = feature.VectorAssembler(
        handleInvalid='skip',
        outputCol='features',
        inputCols=[
            {%- for f in builder_params.features.get_final_features_names() -%}
            '{{f}}',
            {% endfor %}
            {%- if builder_params.reduction.reduction %}'pca_features'
            {%- endif %}
        ])

    # Algorithms definition
    common_params = {'predictionCol': 'prediction',
        {%- if task_type != 'clustering'%}'labelCol': label, {% endif %}
        'featuresCol': 'features'}

    {%- for estimator in builder_params.estimators %}
    {{estimator.generate_code() | indent(width=4, first=False)}}
    {%- endfor %}

    common_stages = features_stages + {% if use_pca %}[pca_va, feature_reducer] + {% endif %} [vec_assembler]
    # CrossValidation/TrainValidationSplit with multiple pipelines in PySpark:
    # See https://stackoverflow.com/a/54992651/1646932
    pipeline = Pipeline(stages=[])

    # Grid parameters (Using: {{builder_params.grid.strategy}})
    {%- for estimator in builder_params.estimators %}
    {%- if builder_params.grid.strategy == 'grid' %}
    {{estimator.generate_hyperparameters_code(task_type=task_type) | indent(width=4, first=False)}}
    {%- else %}
    {{estimator.generate_random_hyperparameters_code() | indent(width=4, first=False)}}
    {%- endif %}

    {%- set constrained = estimator.get_constrained_params() %}
    {%- if constrained %}
    # Remove invalid parameter combinations
    valid_constrained = [
    {%- for p in constrained %}
        {{p}},
    {%- endfor %}
    ]

    grid_{{estimator.var}}_constrained = {}
    for item in grid_{{estimator.var}}:
        valid = True
        key = None
        for validation in valid_constrained:
            if all([item[k] == v for k, v in validation.items()]):
                params = list(item.items())[1:]
                key = tuple((k, item[k]) for k, v in params)
                valid = True
                break
            else:
                valid = False
        if valid:
            grid_{{estimator.var}}_constrained[key] = item

    grid_{{estimator.var}} = grid_{{estimator.var}}_constrained.values()

    {%- endif %}
    {% endfor %}
    # Remove duplicated grid parameters
    candidate_grid = ({%- for estimator in builder_params.estimators -%}
        {%- set variations = estimator.get_variations() %}
        {%- if variations %}
        {%- for variation in variations -%} list(grid_{{estimator.var}}_{{loop.index0}}){% if not loop.last %} + {% endif %}{% endfor %}
        {%- else -%}
        list(grid_{{estimator.var}})
        {%- endif %}
        {%- if not loop.last %} + {% else %}{% endif %}
        {%- if loop.index % 3 == 0 %}
        {# extra line #}
        {%- endif %}
    {%- endfor %})

    distinct_grid = set()
    grid = []
    for row in candidate_grid:
        key = tuple((p, v if not isinstance(v, list) else tuple(v))
            for p, v in row.items() if p.name != 'stages')
        if key not in distinct_grid:
            distinct_grid.add(key)
            grid.append(row)


    if len(grid) == 0:
        raise ValueError(
            "{{gettext('No algorithm or algorithm parameter informed.')}}")
    {%- if builder_params.grid %}
    {{builder_params.grid.generate_code() | indent(width=4, first=False)}}
    {%- endif %}
    return pipeline, grid, label

def main(spark_session: any, cached_state: dict, emit_event: callable):
    """ Run generated code """

    try:
        {%- if task_type != 'clustering' %}
        label_attr = '{{builder_params.features.label.name}}'
        {%- else %}
        label_attr = None
        {%- endif %}
        df = read_data(spark_session)
        {%- if builder_params.sample and builder_params.sample.type %}
        {{builder_params.sample.model_builder_code() | indent(width=8, first=False)}}
        {%- endif %}

        # Missing data handling
        {{builder_params.features.generate_code_for_missing_data_handling() |
                                                                          indent(width=8, first=False)}}

        original_features_names = [
            {%- for f in builder_params.features.features -%}
            '{{ f.name }}'{%- if not loop.last %}, {% endif %}
            {%- endfor -%}
        ]
        final_features_names = [
            {%- for f in builder_params.features.get_final_features_names() -%}
            '{{ f }}'{%- if not loop.last %}, {% endif %}
            {%- endfor -%}
        ]
        # Pipeline definition
        pipeline, grid, label = create_pipeline(
            label_attr, features_names=final_features_names)
        emit = emit_metric(emit_event)

        # Pipeline evaluator
        {%- if builder_params.evaluator %}
        {{builder_params.evaluator.generate_code() | indent(width=8, first=False)}}

        {%- endif %}

        # Train/validation definition
        # It can be a simple train + validation or cross validation
        {%- if builder_params.split %}
        {{builder_params.split.generate_code() | indent(width=8, first=False)}}
        {%- endif %}

        results = executor.fit(df, emit, original_features_names, label_attr)
        results = sorted(results, key=lambda x: x.metric,
                         reverse=evaluator.isLargerBetter())

        if len(results):
            index, status, model, params, metric, ex, ctx, fi = results[0]
            metric_name = evaluator.extractParamMap().get(evaluator.metricName)

            if model:
                features = model.stages[-2].getInputCols()
                df_transformed = model.transform(df)

                # Test if label was indexed with StringIndexer
                stage_indexer_label = next(
                    (s for s in model.stages
                        if isinstance(s, feature.StringIndexerModel)
                        and s.getInputCol() == label_attr), None
                )
                # Revert the label indexing
                if stage_indexer_label:
                    label_reverse = feature.IndexToString(
                            inputCol="prediction",
                            outputCol="prediction2",
                            labels=stage_indexer_label.labels
                    )
                    df_final = (
                        label_reverse
                            .transform(df_transformed)
                            .select(
                                df.columns +
                                [functions.col('prediction2').alias('prediction')]
                            )
                    )
                else:
                    df_final = (df_transformed
                        .select(df.columns + ['prediction']))

                # print(df_final.show())
                # TODO Emit a sample data
                # If set, save the data
                # If set, save the model
                # Generate visualizations
                # Type: {{builder_params.evaluator.task_type}}

                {%- if task_type in ('binary-classification', 'multiclass-classification') %}
                # Convert to RDD of (prediction, label)
                rdd_cm = (df_transformed.select(label, 'prediction')
                       .rdd
                       .map(lambda row:
                            (float(row['prediction']), float(row[label]))
                        )
                )
                multiclass_metrics = MulticlassMetrics(rdd_cm)
                metric_value = {
                    'f1': multiclass_metrics.weightedFMeasure(),
                    'weightedPrecision': multiclass_metrics.weightedPrecision,
                    'weightedRecall': multiclass_metrics.weightedRecall,
                    'accuracy': multiclass_metrics.accuracy,
                }.get(metric_name)

                # Get confusion matrix
                if stage_indexer_label:
                    cm = ConfusionMatrixImageReport(
                        multiclass_metrics.confusionMatrix().toArray(),
                        classes=stage_indexer_label.labels)
                else:
                    classes = (df
                                .select(label_attr)
                                .distinct()
                                .rdd
                                .flatMap(lambda x: x)
                                .collect()
                    )
                    cm = ConfusionMatrixImageReport(
                        multiclass_metrics.confusionMatrix().toArray(),
                        classes=classes)

                content = cm.generate()

                #print('>' * 30, 'confusion matrix', content)
                emit(status='RUNNING', type='HTML',
                    identifier=evaluator.task_id,
                    operation_id=evaluator.operation_id,
                    title=cm.title,
                    message=f'<img alt="[result]" src="data:image/png;base64,{content}"/>')


                {%- if task_type == 'binary-classification' %}
                from pyspark.mllib.evaluation import BinaryClassificationMetrics

                if metric_name == 'areaUnderPR':
                    method_name = 'pr'
                else:
                    method_name = 'roc'

                if 'probability' in df_transformed.columns:
                    rdd_area = (df_transformed.select(label, 'probability')
                        .rdd
                        .map(lambda row:
                             (float(row['probability'][1]), float(row[label]))
                        )
                    )
                    cm = CurveMetrics(rdd_area)
                    points = cm.get_curve(method_name)
                    metric_value = cm.areaUnderROC
                    x_val, y_val = zip(*points)

                    curve_title = "{{gettext('Area under {} curve (AUC = {:1.4f})')}}".format(
                        method_name.upper(), metric_value)
                    content = AreaUnderCurveReport(
                        x_val, y_val, curve_title, method_name).generate(
                            submission_lock
                        )

                    # print('>' * 30 , 'auc', content)
                    emit(status='RUNNING', type='HTML',
                        identifier=evaluator.task_id,
                        operation_id=evaluator.operation_id,
                        title=curve_title,
                        message=f'<img alt="[result]" src="data:image/png;base64,{content}"/>')
                {%- endif %}

                {%- elif task_type == 'regression' %}
                df_transformed = model.transform(df)

                rdd = (df_transformed.select([label_attr, 'prediction'])
                       .rdd
                       .map(lambda row: (
                                row[label_attr], row['prediction'],
                                row[label_attr] - row['prediction']
                            )
                         ).sortBy(lambda x: x[0])
                )
                actual, predicted, residual = zip(*rdd.collect())

                min_value = min(min(actual), min(predicted))
                max_value = max(max(actual), max(predicted))
                identiy = np.linspace(min_value, max_value, len(actual))

                report = PlotlyChartReport()

                sorted_pairs = sorted(zip(actual, predicted))
                sorted_actual, sorted_predicted = (
                    [x for x, _ in sorted_pairs], [y for _, y in sorted_pairs])

                content = report.plot(
                    #gettext('Actual versus Prediction'),
                    '',
                    "{{gettext('Actual')}}",
                    "{{gettext('Prediction')}}",
                    (sorted_actual, sorted_predicted),
                    (identiy, identiy),
                    submission_lock=submission_lock,
                    traces=["{{gettext('Actual versus Prediction')}}",
                            "{{gettext('Identity')}}"],
                        mode=['markers', 'lines']
                )
                # print('>' * 30 , 'actual vs prediction', content)
                emit(status='RUNNING', type='VISUALIZATION',
                    identifier=evaluator.task_id,
                    operation_id=evaluator.operation_id,
                    title="{{gettext('Actual versus Prediction')}}",
                    message=content)

                report = PlotlyChartReport()
                x_ax = list(range(0, len(actual)))
                content_a_p = report.plot(
                    #gettext('Actual and Prediction'),
                    '',
                    "{{gettext('Record #')}}",
                    label_attr,
                    (x_ax, actual),
                    (x_ax, predicted),
                    (x_ax, residual),
                    submission_lock=submission_lock,
                    show_legend=True,
                    traces=["{{gettext('Actual')}}", "{{gettext('Prediction')}}",
                            "{{gettext('Residual')}}"],
                    mode=['lines', 'lines', 'lines']
                )
                emit(status='RUNNING', type='VISUALIZATION',
                    identifier=evaluator.task_id,
                    operation_id=evaluator.operation_id,
                    title="{{gettext('Actual and Prediction')}}",
                    message=content_a_p)

                data = [
                    dict(prediction=x[0], residual=x[1])
                        for x in zip(predicted, residual)
                ]

                report_res = PlotlyChartReport()
                content = report_res.plot_jointplot(
                    predicted, residual,
                    width=800,
                    height=500,
                    n_bins=40,
                    point_size=3,
                    point_color='darkblue',
                    point_opacity=0.5,
                    hist_color='royalblue',
                    hist_opacity=0.8,
                    x_label="{{gettext('Prediction')}}",
                    y_label="{{gettext('Residual')}}",
                    title='' # gettext('Prediction versus Residual')
                )
                emit(status='RUNNING', type='VISUALIZATION',
                    identifier=evaluator.task_id,
                    operation_id=evaluator.operation_id,
                    title="{{gettext('Prediction versus Residual')}}",
                    message=content)

                {# old
                report2 = SeabornChartReport()
                content = report2.jointplot(
                    data, 'prediction', 'residual',
                    "{{gettext('Prediction versus Residual')}}",
                    "{{gettext('Prediction'),}}"
                    "{{gettext('Residual')}}",
                    submission_lock=submission_lock
                )
                #print('>' * 30, 'prediction vs residual', content)
                emit(status='RUNNING', type='HTML',
                    identifier=evaluator.task_id,
                    operation_id=evaluator.operation_id,
                    title="{{gettext('Prediction versus Residual')}}",
                    message=f'<img alt="[result]" src="data:image/png;base64,{content}"/>')
                #}
                {%- elif task_type == 'clustering' %}
                # Prepare k-means models
                km_models = sorted(
                    filter(
                        lambda model: isinstance(model[1], clustering.KMeansModel),
                        map(lambda result: (result.index, result.model.stages[-1]),
                            results),
                    ),
                    key=lambda model: model[1].extractParamMap().get(model[1].k)
                )
                print('*' * 20)
                print([m[1].summary.trainingCost for m in km_models])
                print('*' * 20)
                {%- endif%}


                # Send a sample of data
                css_classes = 'table table-sm table-striped mx-3 table-smallest'
                emit(status='RUNNING', type='HTML',
                        identifier=evaluator.task_id,
                        operation_id=evaluator.operation_id,
                        title="{{gettext('Resulting data (sample)')}}",
                        message=(df_final.limit(50).toPandas()
                                 .to_html(index=False, classes=css_classes))
                )

            # Send information about best result
            message = {
                'best': params, 'metric_value': metric,
                'larger_better': evaluator.isLargerBetter(),
                'index': ctx.index, 'metric_name': metric_name,
                'estimator': ctx.name, 'task_name': ctx.task_name,
                'train_size': executor.train_size,
                'test_size': executor.test_size,
                'model_count': len(results)
            }
            emit(status='COMPLETED', type='OTHER',
                identifier=evaluator.task_id,
                operation_id=evaluator.operation_id,
                title='{{ builder_params.evaluator.task_name}}',
                message=message)
            # print(message){#-
        #for status, model, params, metric, ex, ctx, fi in results:
        #    pass
            # print(model.stages[-1], [(p, v) for p, v in params.items()], metric)
            # ml_model = model.stages[-1]
            # features = model.stages[-2].getInputCols()
            # print('#' * 20)
            # print('Features', features)
            # importance = get_feature_importance_score(ml_model, features)
            # print('#' * 20)
            # print(feature_assembler.metadata)
            # print(ml_model.metadata["ml_attr"]["attrs"].values())
        #}
    except Exception as e:
        spark_session.sparkContext.cancelAllJobs()
        traceback.print_exc(file=sys.stderr)
        if not dataframe_util.handle_spark_exception(e):
            raise
