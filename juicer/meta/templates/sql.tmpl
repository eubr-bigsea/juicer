#!/usr/bin/env python
"""
Auto-generated Spark code from Lemonade Workflow
(c) Speed Labs - Departamento de Ciência da Computação
    Universidade Federal de Minas Gerais
More information about Lemonade to be provided.
"""
import traceback
import sys
import os
from pyspark.sql import types # type: ignore
from juicer.util import dataframe_util
from termcolor import colored
from gettext import gettext
{%- set use_hwc = workflow.forms.get('$meta', {}).get('value', {}).get('use_hwc') in [True, 'true'] %}
{%- if use_hwc %}{{transpiler.add_import('from pyspark_llap import HiveWarehouseSession')}}{%- endif %}

{%- for imps in transpiler.imports %}
{{imps}}
{%- endfor %}

{%- set builder_params = transpiler.transpiler.prepare_sql_workflow_parameters(instances) %}

{%- if use_hwc %}

def get_hwc_connection(spark_session):
  """
  Gets a connection to Hive Warehouse Connector (HWC) (creates it if necessary).

  Returns:
      The HWC connection object.
  """
  hive = None
  def create_connection():
    nonlocal hive  # Modify the variable within the enclosing function
    if spark_session.conf.get('spark.sql.hive.hiveserver2.jdbc.url') is None:
        raise ValueError(gettext('Cluster is not configured for Hive Warehouse')
    hive = HiveWarehouseSession.session(spark_session).build()
  if not hive:
    create_connection()
  return hive
{%- endif %}
{%- for reader in builder_params.readers %}
{%- set name = transpiler.text_to_identifier(reader.task.name) %}

def read_data_{{name.lower()}}(spark_session):
    """ Read input data."""
    {{reader.sql_code() |  indent(width=4, first=False) }}
    return df
{%- endfor %}

{%- for sql in builder_params.sqls %}

def execute_sql_{{sql.order}}(spark_session):
    """ Execute the query. """
    {{sql.sql_code() |  indent(width=4, first=False) }}
    {%- if sql.parameters.save %}
    {%- endif %}
    return result
{%- endfor %}

def main(spark_session: any, cached_state: dict, emit_event: callable):
    """ Run generated code """

    try:
        {%- for reader in builder_params.readers %}
        {%- set name = transpiler.text_to_identifier(reader.task.name) %}
        (hash, result) = cached_state.get(
            '{{reader.task.id}}', (None, None))
        if hash == '{{reader.parameters.hash}}':
            df_{{loop.index0}} = result
            print(colored('Using cache', 'green'))
        else:
            result = read_data_{{name}}(spark_session)
            cached_state['{{reader.task.id}}'] = (
                '{{reader.parameters.hash}}', result)
            df_{{loop.index0}} = result
        df_{{loop.index0}}.createOrReplaceTempView('`{{reader.task.name}}`')

        {%- endfor %}

        # Queries
        {%- for sql in builder_params.sqls %}
        (hash, result) = cached_state.get(
            '{{sql.task.id}}', (None, None))
        if hash == '{{sql.parameters.hash}}':
            result_{{sql.order}} = result
            print(colored('Using cache', 'green'))
        else:
            result = execute_sql_{{sql.order}}(spark_session)
            cached_state['{{sql.task.id}}'] = (
                '{{sql.parameters.hash}}', result)
            result_{{sql.order}} = result

        {%- if sql.task.name %}
        result_{{sql.order}}.createOrReplaceTempView('`{{sql.task.name}}`')
        {%- endif %}
        {%- endfor %}

        # if last command is a SELECT one, display a sample
        {%- if builder_params.sqls %}
        {%- set last_result = 'result_' + (builder_params.sqls|last).order|string %}
        if {{last_result}}.columns:
            task_id = '{{ builder_params.sqls[-1].task.id}}'
            dataframe_util.emit_sample_data_explorer(
                task_id, {{last_result}}, emit_event, 'output')
        {%- endif %}
        return cached_state

    except Exception as e:
        spark_session.sparkContext.cancelAllJobs()
        traceback.print_exc(file=sys.stderr)
        if not dataframe_util.handle_spark_exception(e):
            raise
