#!/usr/bin/env python
"""
Auto-generated Spark code from Lemonade Workflow
(c) Speed Labs - Departamento de Ciência da Computação
    Universidade Federal de Minas Gerais
More information about Lemonade to be provided.
"""
import traceback
import sys
from pyspark.sql import types # type: ignore
from juicer.util import dataframe_util

{%- for imps in transpiler.imports %}
{{imps}}
{%- endfor %}

{%- set builder_params = transpiler.transpiler.prepare_sql_workflow_parameters(instances) %}
{%- for reader in builder_params.readers %}
{%- set name = transpiler.text_to_identifier(reader.task.name) %}

def read_data_{{name}}(spark_session):
    """ Read input data."""
    {{reader.sql_code() |  indent(width=4, first=False) }}
    return df
{%- endfor %}

{%- for sql in builder_params.sqls %}

def execute_sql_{{sql.order}}(spark_session):
    """ Execute the query. """
    {{sql.sql_code() |  indent(width=4, first=False) }}
{%- endfor %}

def main(spark_session: any, cached_state: dict, emit_event: callable):
    """ Run generated code """

    try:
        {%- for reader in builder_params.readers %}
        {%- set name = transpiler.text_to_identifier(reader.task.name) %}
        (hash, result) = cached_state.get(
            '{{reader.task.id}}', (None, None))
        if hash == '{{reader.parameters.hash}}':
            df_{{loop.index0}} = result 
        else:
            result = read_data_{{name}}(spark_session)
            cached_state['{{reader.task.id}}'] = (
                '{{reader.parameters.hash}}', result)
            df_{{loop.index0}} = result 
        df_{{loop.index0}}.createOrReplaceTempView('`{{reader.task.name}}`')
        
        {% endfor %}

        # Queries
        {%- for sql in builder_params.sqls %}
        (hash, result) = cached_state.get(
            '{{sql.task.id}}', (None, None))
        if hash == '{{sql.parameters.hash}}':
            result_{{sql.order}} = result 
        else:
            result = execute_sql_{{sql.order}}(spark_session)
            cached_state['{{sql.task.id}}'] = (
                '{{sql.parameters.hash}}', result)
            result_{{sql.order}} = result 
            result_{{sql.order}}.explain()

        {%- if sql.task.name %}
        result_{{sql.order}}.createOrReplaceTempView('`{{sql.task.name}}`')
        {%- endif %}
        {%- endfor %}

        # if last command is a SELECT one, display a sample
        {%- set last_result = 'result_' + (builder_params.sqls|length -1)|string %}
        if {{last_result}}.columns:
            task_id = '{{ builder_params.sqls[-1].task.id}}'
            dataframe_util.emit_sample_data_explorer(
                task_id, {{last_result}}, emit_event, 'output')
        return cached_state

    except Exception as e:
        spark_session.sparkContext.cancelAllJobs()
        traceback.print_exc(file=sys.stderr)
        if not dataframe_util.handle_spark_exception(e):
            raise
