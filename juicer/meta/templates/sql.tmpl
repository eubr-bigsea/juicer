#!/usr/bin/env python
"""
Auto-generated Spark code from Lemonade Workflow
(c) Speed Labs - Departamento de Ciência da Computação
    Universidade Federal de Minas Gerais
More information about Lemonade to be provided.
"""
import traceback
import sys
import os
from pyspark.sql import types # type: ignore
from juicer.util import dataframe_util
from termcolor import colored
from gettext import gettext
import functools

{%- set use_hwc = workflow.forms.get('$meta', {}).get('value', {}).get('use_hwc') in [True, 'true'] %}
{%- if use_hwc %}{{transpiler.add_import('from pyspark_llap import HiveWarehouseSession') or ''}}{%- endif %}

{%- for imps in transpiler.imports %}
{{imps}}
{%- endfor %}

{%- set code_library = transpiler.transpiler.get_source_code_library_code(workflow) %}
{%- if code_library %}

# Code from Code Libraries. They must be Python functions.
{%- for code in code_library %}
{{code}}
{%- endfor %}
{%- endif %}

{%- set builder_params = transpiler.transpiler.prepare_sql_workflow_parameters(instances) %}

{%- if use_hwc %}

def get_hwc_connection(spark_session):
  """
  Gets a connection to Hive Warehouse Connector (HWC) (creates it if necessary).

  Returns:
      The HWC connection object.
  """
  hive = None
  def create_connection():
    nonlocal hive  # Modify the variable within the enclosing function
    if spark_session.conf.get('spark.sql.hive.hiveserver2.jdbc.url') is None:
        raise ValueError(gettext('Cluster is not configured for Hive Warehouse'))
    hive = HiveWarehouseSession.session(spark_session).build()
  if not hive:
    create_connection()
  return hive
{%- endif %}

{%- for reader in builder_params.readers %}
{%- set name = transpiler.text_to_identifier(reader.task.name) %}

def read_data_{{name.lower()}}(spark_session, context):
    """ Read input data."""
    {{reader.sql_code() |  indent(width=4, first=False) }}
    return df
{%- endfor %}

{%- for cell in builder_params.cells %}
{%- if cell.enabled %}

def execute_cell_{{cell.order}}(spark_session, context, emit):
{%- if cell.task.operation.slug == 'execute-sql' %}
    """ Execute the query. """
    df = None
    success = True
    {{cell.sql_code() |  indent(width=4, first=False) }}
    return (df, success)
{%- else %}
    """ Execute Python command. """

    # Allow to send messages to user interface
    notify = functools.partial(
        emit, name='user message',
        status='RUNNING', type='TEXT',
        identifier='{{cell.task.id}}',
        task={'id': '{{cell.task.id}}'},
    )
    {{cell.sql_code() |  indent(width=4, first=False) }}
{%- endif %}
{%- endif %}
{%- endfor %}

def main(spark_session: any, cached_state: dict, emit_event: callable):
    """ Run generated code """
    context = {}

    try:
        # TODO: Evaluate other approach to register Java's UDF
        spark_session.udf.registerJavaFunction(
            "validate_codes",
            "br.ufmg.dcc.lemonade.udfs.ValidateCodesUDF",
            types.BooleanType())
        spark_session.udf.registerJavaFunction(
            "date_patterning",
            "br.ufmg.dcc.lemonade.udfs.DatePatterningUDF",
            types.StringType())
        spark_session.udf.registerJavaFunction(
            "physical_or_legal_person",
            "br.ufmg.dcc.lemonade.udfs.PhysicalOrLegalPersonUDF",
            types.StringType())
        spark_session.udf.registerJavaFunction(
            "strip_accents",
            "br.ufmg.dcc.lemonade.udfs.StripAccentsUDF",
            types.StringType())
    except Exception as e:
        traceback.print_exc(file=sys.stderr)

    try:
        # Keep the names of valid temporary tables only
        tables_to_keep = [
            {%- for reader in builder_params.readers %}
            '`{{reader.task.name}}`',
            {%- endfor%}
            {%- for cell in builder_params.cells %}
            {%- if cell.task.name and cell.supports_cache and cell.task.operation.slug == 'execute-sql' %}
            '`{{cell.task.name}}`',
            {%- endif %}
            {%- endfor%}
        ]
        all_temp_tables = spark_session.catalog.listTables()
        # Filter out the tables that are not in the list to keep
        tables_to_drop = [table.name for table in all_temp_tables if
                          table.name not in tables_to_keep]

        # Drop the unwanted temporary tables
        for table_name in tables_to_drop:
            spark_session.catalog.dropTempView(table_name)

        {%- for reader in builder_params.readers %}
         # Data source {{loop.index}}
        emit_event('update task', message=_('Task running'),
            status='RUNNING', identifier='{{reader.task.id}}')
        {%- set name = transpiler.text_to_identifier(reader.task.name) %}
        (hash, result) = cached_state.get(
            '{{reader.task.id}}', (None, None))
        if hash == '{{reader.parameters.hash}}':
            df_{{loop.index0}} = result
            print(colored('Using cache', 'green'))
        else:
            result = read_data_{{name}}(spark_session, context)
            cached_state['{{reader.task.id}}'] = (
                '{{reader.parameters.hash}}', result)
            df_{{loop.index0}} = result
        df_{{loop.index0}}.createOrReplaceTempView('`{{reader.task.name}}`')
        emit_event('update task', message=_('Task completed'),
            status='COMPLETED', identifier='{{reader.task.id}}')
        {%- endfor %}

        results = []
        {%- for cell in builder_params.cells %}
        {%- if cell.enabled %}
        emit_event('update task', message=_('Task running'),
            status='RUNNING', identifier='{{cell.task.id}}')
        # Cell {{loop.index}}
        {%- if cell.supports_cache %}
        (hash, result) = cached_state.get(
            '{{cell.task.id}}', (None, None))
        if hash == '{{cell.parameters.hash}}':
            result_{{cell.order}} = result
            df = result
            print(colored('Using cache', 'green'))
        else:
            df, success = execute_cell_{{cell.order}}(spark_session, context, emit_event)
            if df is not None:
                cached_state['{{cell.task.id}}'] = (
                    '{{cell.parameters.hash}}', df)
                result_{{cell.order}} = df
            else:
                result_{{cell.order}} = None
        results.append([df, '{{cell.task.id}}'])
        {%- elif cell.task.operation.slug == 'execute-sql'%}
        # Does not support caching results
        df, success = execute_cell_{{cell.order}}(spark_session, context, emit_event)
        result_{{cell.order}} = df
        results.append([df, '{{cell.task.id}}'])
        {%- else %}
        # Python code cell
        execute_cell_{{cell.order}}(spark_session, context, emit_event)
        {%- endif %}

        {%- if cell.task.name and cell.supports_cache %}
        if df is not None:
            result_{{cell.order}}.createOrReplaceTempView('`{{cell.task.name}}`')
        {%- endif %}
        emit_event('update task', message=_('Task completed'),
            status='COMPLETED', identifier='{{cell.task.id}}')
        {%- endif %}
        {%- endfor %}

        if results and results[-1][0].columns:
            task_id = results[-1][1]
            dataframe_util.emit_sample_data_explorer(
                task_id, results[-1][0], emit_event, 'output')

        return cached_state

    except Exception as e:
        spark_session.sparkContext.cancelAllJobs()
        traceback.print_exc(file=sys.stderr)
        if not dataframe_util.handle_spark_exception(e):
            raise
